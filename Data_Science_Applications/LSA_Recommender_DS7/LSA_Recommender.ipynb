{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mysixenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3d88cacd8b745d4fbed2830424fce0451ea91f459b833c1d75c3abe001a2f0c4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Natural Language Processing: Topic Modeling with Latent Semantic Analysis\n",
    "\n",
    "Background: \n",
    "\n",
    "In NLP, one important application is about Topic Modeling, with which we try to capture the underlying themes that appear in a group of documents. Basically, we have two approaches:\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA) -  generate k topics by first assigning each word to a random topic, then iteratively updating assignments based on parameters $\\alpha$, the mix of topics per document, and $\\beta$, the distribution of words per topic.\n",
    "\n",
    "- Latent Semantic Analysis (LSA) - identifies patterns using TF-IDF scores and reduces data to k dimensions through SVD. In other words, given a corpus of articles, we want to create a term-document-type of matrix, for which we can do SVD analysis.\n",
    "\n",
    "Goal:\n",
    "\n",
    "In this project, we want to empoly ***LSA***, trainig LSA-based recommender systems. And see whether they can promote what we are looking for from a collection of articles and from a collection of jokes:)\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Environment Setup"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\xxxli\\Anaconda3\\envs\\mysixenv\\lib\\site-packages\\numpy\\_distributor_init.py:32: UserWarning: loaded more than 1 DLL from .libs:\nC:\\Users\\xxxli\\Anaconda3\\envs\\mysixenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.IPBC74C7KURV7CB2PKT5Z5FNR3SIBV4J.gfortran-win_amd64.dll\nC:\\Users\\xxxli\\Anaconda3\\envs\\mysixenv\\lib\\site-packages\\numpy\\.libs\\libopenblas.WCDJNK7YVMPZQ2ME2ZZHJJRJ3JIKNDB7.gfortran-win_amd64.dll\n  stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse.csr as csr\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "Let's do some preworks to understand NLP first:\n",
    "## Bag-of-Words matrix: by term frequency\n",
    "\n",
    "Following the <a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html\">Sklearn Feature-extraction documentation page</a>\n",
    "\n",
    "- I start with a given *corpus* of $D = 4$ documents.\n",
    "- I preprocess each document and convert it into a list of terms (features)\n",
    "    - by lowercasing first\n",
    "    - accepting only word patterns (defined via [RegEx](https://www.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html))\n",
    "- then I form the ***Count-Vectorizer term-frequency*** matrix defined as:\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d)\\equiv{CF}_{d,t} = \\text{number of times that the term }t\\text{ occurs in document }d\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# preprocessing\n",
    "# a default type of pattern for stop words\n",
    "tpatterns = [\n",
    "    '(?u)\\\\b\\\\w\\\\w+\\\\b', #default tpatterns[0]\n",
    "    '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b', #tpatterns[1]\n",
    "    '\\\\w',#tpatterns[2]\n",
    "    '\\\\w+',#tpatterns[3]\n",
    "    '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b|\\\\b[0-9]\\\\b'#tpatterns[4]\n",
    "]\n",
    "\n",
    "# instantiate a contvectorizer, which just does the TF transformation\n",
    "vectorizer = CountVectorizer(token_pattern=tpatterns[0])\n",
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# start with four documents\n",
    "corpus = [\n",
    "    'This is the the first first document abra.',\n",
    "    'This is the second second document cadabra.',\n",
    "    'And the third one 3.',\n",
    "    'Is this the first document 4?',\n",
    "]\n",
    "# list of string\n",
    "# map corpus onto a matrix 4x11: 4 documents, 11 unique words/terms\n",
    "X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "X_corpus_docterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 2, 1, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "X_corpus_docterm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abra',\n",
       " 'and',\n",
       " 'cadabra',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# note the effect of modifying the token_pattern above....\n",
    "features = vectorizer.get_feature_names() \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   abra  and  cadabra  document  first  is  one  second  the  third  this\n",
       "0     1    0        0         1      2   1    0       0    2      0     1\n",
       "1     0    0        1         1      0   1    0       2    1      0     1\n",
       "2     0    1        0         0      0   0    1       0    1      1     0\n",
       "3     0    0        0         1      1   1    0       0    1      0     1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abra</th>\n      <th>and</th>\n      <th>cadabra</th>\n      <th>document</th>\n      <th>first</th>\n      <th>is</th>\n      <th>one</th>\n      <th>second</th>\n      <th>the</th>\n      <th>third</th>\n      <th>this</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# visualize the TF matrix I just made\n",
    "pd.DataFrame(X_corpus_docterm.toarray(),columns = np.array(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(?u)\\b\\w\\w+\\b\n['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n(?u)\\b[a-zA-Z]\\w+\\b\n['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n\\w\n['3', '4', 'a', 'b', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n\n\n\\w+\n['3', '4', 'abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n(?u)\\b[a-zA-Z]\\w+\\b|\\b[0-9]\\b\n['3', '4', 'abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n"
     ]
    }
   ],
   "source": [
    "# For each defined regex pattern, take a look the outputs\n",
    "for tp in tpatterns:\n",
    "    vectorizer.token_pattern = tp\n",
    "    print(vectorizer.token_pattern)\n",
    "    X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print(features)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Our TF matrix: \n [[1 0 0 1 2 1 0 0 2 0 1]\n [0 0 1 1 0 1 0 2 1 0 1]\n [0 1 0 0 0 0 1 0 1 1 0]\n [0 0 0 1 1 1 0 0 1 0 1]]\nEach column corresponds:\n ['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "# So, this is what we want to do here:\n",
    "vectorizer.token_pattern = tpatterns[0] #back to the default\n",
    "X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "CV = X_corpus_docterm.toarray()\n",
    "print('Our TF matrix: \\n', CV)\n",
    "print('Each column corresponds:\\n',features)"
   ]
  },
  {
   "source": [
    "For example, the first document \"This is the the first first document abra.\" contains the word ***first*** twice, so we have entry (0,4) is 2."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### What if we want to vectorize a document in the test set, in which there could even be a document with words that not even encountered before in the training set?\n",
    "\n",
    "- Call the `.transform()` method of the trained vectorizer on testing set\n",
    "- We see that the new word will not appear in our matrix"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "doc: rain\n[0 0 0 0 0 0 0 0 0 0 0]\ndoc: The a yoghurt\n[0 0 0 0 0 0 0 0 1 0 0]\ndoc: three two one hurray\n[0 0 0 0 0 0 1 0 0 0 0]\ndoc: and abra document is first cadabra\n[1 1 1 1 1 1 0 0 0 0 0]\ndoc: one-third abra is the first cadabra and this document a second\n[1 1 1 1 1 1 1 1 1 1 1]\n\n Resulting test TF matrix:\n [[0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 1 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0]\n [1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "def docs2vec(docs, vectorizer):\n",
    "    \"\"\" \n",
    "    docs: testing set of documents\n",
    "    vectorizer: trained CountVectorizer\n",
    "    \"\"\"\n",
    "    return vectorizer.transform(docs)\n",
    "\n",
    "# testing set\n",
    "docs_test = [\n",
    "    'rain',\n",
    "    'The a yoghurt',\n",
    "    'three two one hurray',\n",
    "    'and abra document is first cadabra',\n",
    "    'one-third abra is the first cadabra and this document a second',\n",
    "]\n",
    "\n",
    "# for each testing doc, take a look at the result\n",
    "for doc in docs_test:\n",
    "    print(\"doc:\",doc)\n",
    "    print(docs2vec([doc], vectorizer).toarray()[0])\n",
    "    \n",
    "# fit on testing set\n",
    "X_docterm_test = docs2vec(docs_test, vectorizer)\n",
    "print('\\n Resulting test TF matrix:\\n', X_docterm_test.toarray())"
   ]
  },
  {
   "source": [
    "## TF-IDF: term frequency inverse document frequency\n",
    "### Problems with Bag-of-Words TF matrix:\n",
    "\n",
    "- The document vectors are **not normalized**\n",
    "    - so can't really compare documents\n",
    "- The document vectors contain many common english words containing **no information**\n",
    "    - ideally we want to remove those, e.g. 'the', 'is', etc\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution: TF-IDF vectorizer ([The TF-idf section in the Scikit-Learn feature extraction manual](http://scikit-learn.org/stable/modules/feature_extraction.html))\n",
    "\n",
    "TF-IDF matrix is defined as (when `smooth_idf=True`)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{tf-idf}(t,d) &\\equiv{\\text{tf}}(t,d)\\times\\text{idf}(t)\\\\\n",
    "\\text{idf}(t)&\\equiv\\log\\frac{1+n_d}{1+\\text{df}(d,t)} + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\text{df}(d,t)$ is the number of documents containing feature $t$\n",
    "- $n_d$ is the number of documents in our corpus\n",
    "- the rows of the tf-idf matrix are normalized to have unit norm (either $L_1$ or $L_2$)\n",
    "    - this way we can compare documents by the norm of their doc2vec overlaps"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Let's do this in practice:\n",
    "\n",
    "Firstly, create a corpus with various levels of repetition of its terms:\n",
    "- $N$ is the number of documents in our corpus. We can play with it to see why TF-IDF is a useful tool!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['blah abra cadabra',\n",
       " 'blah abra cadabra',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "N = 10\n",
    "corpus = np.reshape(['blah', 'abra', 'cadabra'] * N, (N,3))\n",
    "corpus[2:,2] = ''\n",
    "corpus[int(N/2):,1] = ''\n",
    "corpus = [' '.join(corpus[i]) for i in range(N)]\n",
    "# print('\\n'.join(corpus[:10]))\n",
    "corpus[:10]"
   ]
  },
  {
   "source": [
    "Initiate TF-IDF Vectorizer:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english', # add stop words\n",
    "    norm='l2', # each output vector has l2 norm equal to 1\n",
    "    use_idf=True)"
   ]
  },
  {
   "source": [
    "Train:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_corpus_tfidf=vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "source": [
    "Let's see what is the differnce?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "       abra      blah   cadabra\n",
       "0  0.539398  0.335836  0.772181\n",
       "1  0.539398  0.335836  0.772181\n",
       "2  0.848908  0.528541  0.000000\n",
       "3  0.848908  0.528541  0.000000\n",
       "4  0.848908  0.528541  0.000000\n",
       "5  0.000000  1.000000  0.000000\n",
       "6  0.000000  1.000000  0.000000\n",
       "7  0.000000  1.000000  0.000000\n",
       "8  0.000000  1.000000  0.000000\n",
       "9  0.000000  1.000000  0.000000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abra</th>\n      <th>blah</th>\n      <th>cadabra</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.539398</td>\n      <td>0.335836</td>\n      <td>0.772181</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.539398</td>\n      <td>0.335836</td>\n      <td>0.772181</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.848908</td>\n      <td>0.528541</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.848908</td>\n      <td>0.528541</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.848908</td>\n      <td>0.528541</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "pd.DataFrame(X_corpus_tfidf.toarray(), columns = vectorizer.get_feature_names()).head(10)"
   ]
  },
  {
   "source": [
    "#### We found\n",
    "- When we vary number of documents $N$:\n",
    "    - When $N$ is larger, the weight of 'cadabra' becomes larger.\n",
    "    - WHen $N$ is small, the weight of 'cadabra' becomes smaller.\n",
    "\n",
    "- Similarly, the weights of 'blah' and 'abra' also change due to the change of $N$. \n",
    "    - This is what TF-IDF does! Measuring the importance of words, not only in terms of the word occurences in one document, but also in terms of how frequent the document that contains the words appears in our corpus.\n",
    "\n",
    "- Whereever, we always get the least weights on 'blah', this is the least relevent word to look at since every document has it, but 'cadabra' really mean something when it occurs since only 2 documents contain it."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "---------- \n",
    "It's time to construct our Recommendation System! Boom-ya!\n",
    "\n",
    "## Latent Semantic Analysis (Truncated SVD on the TF-IDF matrix)\n",
    "\n",
    "References:\n",
    "\n",
    "- [Scikit-Learn's Reuters Dataset TF-IDF + K-NN classification example](http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html)\n",
    "- [Chris McCormic's LSA tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
    "- [Chris McCormic's GitHub Page](\"https://github.com/chrisjmccormick/LSA_Classification\")\n",
    "\n",
    "Dataset: \n",
    "\n",
    "- the Reuters Articles Corpus\n",
    "- The original Reuter's 21578 dataset is part of the [UCI-ML repository](http://archive.ics.uci.edu/ml/machine-learning-databases) and can be found [here](\"http://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz). \n",
    "- However, in this notebook, I will use the already pre-processed version in [Chris McCormic's github page](https://github.com/chrisjmccormick/LSA_Classification/tree/master/data). Note that for each document, it has several labels. And with this dataset, we can easily get features and labels that are splitted into training and testing sets, respectively as follows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import data & Explore:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Data Importing ...\n<class 'list'> <class 'list'>\nNumber of train docs: 4743 \nNumber of test docs: 4858\nNumber of train labels: 4743 \nNumber of test labels: 4858\n"
     ]
    }
   ],
   "source": [
    "filepath = r\"C:\\Users\\xxxli\\Desktop\\ResumeProjects\\Data_Science_Applications\\LSA_Recommender_DS7\\raw_text_dataset.pickle\"\n",
    "raw_text_dataset = pickle.load(open(filepath, \"rb\"))\n",
    "corpus_train, labels_train = raw_text_dataset[0], raw_text_dataset[1] \n",
    "corpus_test, labels_test = raw_text_dataset[2], raw_text_dataset[3]\n",
    "print('Data Importing ...')\n",
    "print(type(corpus_train), type(labels_train)) # list of string\n",
    "print('Number of train docs:', len(corpus_train), '\\nNumber of test docs:', len(corpus_test))\n",
    "print('Number of train labels:', len(labels_train), '\\nNumber of test labels:', len(labels_test))"
   ]
  },
  {
   "source": [
    "Randomly pick a document and take a look at its content and labels/tags:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nThis is how a article  2759  looks like:\n\n EC FARM LIBERALISATION SEEN HURTING THAI TAPIOCA\n\nAny European Community decision to liberalise farm trade policy would hurt Thailand's tapioca industry, said Ammar Siamwalla, an agro-economist at the Thailand Development Research Institute (TDRI). He told a weekend trade seminar here that any EC move to cut tariff protection for EC grains would make many crops more competitive than tapioca in the European market. The EC is the largest buyer of Thai tapioca, absorbing more than two thirds of the\n\nAnd these are its topic labels or tags:\n\n ['tapioca', 'meal-feed', 'thailand', 'ec']\n"
     ]
    }
   ],
   "source": [
    "n = np.random.choice(len(corpus_train))\n",
    "\n",
    "print('\\nThis is how a article ', n,' looks like:\\n\\n', \n",
    "      corpus_train[n][:500])\n",
    "\n",
    "print('\\nAnd these are its topic labels or tags:\\n\\n', \n",
    "      labels_train[n][:500])"
   ]
  },
  {
   "source": [
    "### Reconstruct train-test sets:\n",
    "We want to use all 10k articles as training set. And\n",
    "- make recommendations based on key words\n",
    "- make recommendations based on a self-input list of documents"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Setup train set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "number of documents: 9601\n"
     ]
    }
   ],
   "source": [
    "corpus = corpus_train + corpus_test\n",
    "corpus_train = corpus\n",
    "print('number of documents:', len(corpus_train))"
   ]
  },
  {
   "source": [
    "### Setup target word:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word = \"cocoa\""
   ]
  },
  {
   "source": [
    "### Setup our target list of documents:\n",
    "* doc1: “Jabberwocky”\n",
    "* doc2: “buy MSFT sell AAPL hold Brent”\n",
    "* doc3: “bullish stocks”\n",
    "* doc4: “Some random forests produce deterministic losses”\n",
    "\n",
    "For each of the following doc strings, calculate their corresponding vectors:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample = ['Jabberwocky',\n",
    "               'buy MSFT sell AAPL hold Brent',\n",
    "               'bullish stocks',\n",
    "               'Some random forests produce deterministic losses']"
   ]
  },
  {
   "source": [
    "### TF-IDF vectorizer step:\n",
    "The TfidfVectorizer below does the following:\n",
    "\n",
    "- **`stop_words='english'`**: Strips out english “stop words”, e.g. frequently occuring english words.\n",
    "- **`max_df=0.5`**: Filters out terms that occur in more than half of the docs.\n",
    "- **`min_df=2`**: Filters out terms that occur in only one document.\n",
    "- **`max_features=10000`**: Selects 10,000 most frequently occuring words in the corpus.\n",
    "- **`norm='l2'`**: Normalizes the vector to account for the effect of document length on the tf-idf values. In other words, each output row will have unit norm, either: \n",
    "    * 'l2': Sum of squares of vector elements is 1. The cosine similarity between two vectors is their dot product when l2 norm has been applied. \n",
    "    * 'l1': Sum of absolute values of vector elements is 1.\n",
    "- **`use_idf=True`**: Enable inverse-document-frequency reweighting. \n",
    "- **`analyzer='word'`**: Whether the feature should be made of word or character n-grams. \n",
    "- **`token_pattern='(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'`**: Regular expression denoting what constitutes a “token”, only used if analyzer == 'word'. The default regexp default=r”(?u)\\b\\w\\w+\\b” selects tokens of 2 or more alphanumeric characters (punctuation is completely ignored and always treated as a token separator).\n",
    "\n",
    "Note: Play around to see what kind of ***`token_pattern`*** actually works the best for for this corpus; How it changes changes the output below!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=10000,\n",
    "    min_df=2,\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "    # token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "    )"
   ]
  },
  {
   "source": [
    "Train:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "IDF vector: [8.56028878 8.78343233 8.09028515 ... 8.09028515 8.78343233 7.46167649]\n"
     ]
    }
   ],
   "source": [
    "X_train_tfidf = vectorizer.fit_transform(corpus_train)\n",
    "print('IDF vector:', vectorizer.idf_) # The inverse document frequency (IDF) vector; only defined if use_idf is True.\n",
    "# X_train_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "first 10 features: ['a300', 'a320', 'a330', 'a340', 'aa', 'aaa', 'aapl', 'ab', 'abandon', 'abandoned']\nlast 10 features: ['zim', 'zimbabwe', 'zinc', 'ziyang', 'zoete', 'zone', 'zones', 'zorinsky', 'zuckerman', 'zurich']\ntrained TFIDF matrix shape: (9601, 10000)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   a300  a320  a330  a340   aa  aaa  aapl   ab  abandon  abandoned  ...  zim  \\\n",
       "0   0.0   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0  ...  0.0   \n",
       "1   0.0   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0  ...  0.0   \n",
       "2   0.0   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0  ...  0.0   \n",
       "3   0.0   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0  ...  0.0   \n",
       "4   0.0   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0  ...  0.0   \n",
       "\n",
       "   zimbabwe  zinc  ziyang  zoete      zone  zones  zorinsky  zuckerman  zurich  \n",
       "0       0.0   0.0     0.0    0.0  0.046785    0.0       0.0        0.0     0.0  \n",
       "1       0.0   0.0     0.0    0.0  0.000000    0.0       0.0        0.0     0.0  \n",
       "2       0.0   0.0     0.0    0.0  0.000000    0.0       0.0        0.0     0.0  \n",
       "3       0.0   0.0     0.0    0.0  0.000000    0.0       0.0        0.0     0.0  \n",
       "4       0.0   0.0     0.0    0.0  0.000000    0.0       0.0        0.0     0.0  \n",
       "\n",
       "[5 rows x 10000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a300</th>\n      <th>a320</th>\n      <th>a330</th>\n      <th>a340</th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aapl</th>\n      <th>ab</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>...</th>\n      <th>zim</th>\n      <th>zimbabwe</th>\n      <th>zinc</th>\n      <th>ziyang</th>\n      <th>zoete</th>\n      <th>zone</th>\n      <th>zones</th>\n      <th>zorinsky</th>\n      <th>zuckerman</th>\n      <th>zurich</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.046785</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 10000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "print('first 10 features:', vectorizer.get_feature_names()[:10])\n",
    "print('last 10 features:', vectorizer.get_feature_names()[-10:])\n",
    "print('trained TFIDF matrix shape:', X_train_tfidf.shape)\n",
    "pd.DataFrame(X_train_tfidf.toarray(), columns = vectorizer.get_feature_names()).head()"
   ]
  },
  {
   "source": [
    "Let's find an article in `X_train_tfidf` that contains a word \"cocao':"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "target word: cocoa\n34 documents found.\nExample:\ndocument index: 6767\n-----\nCOCOA DELEGATES OPTIMISTIC ON BUFFER STOCK RULES\n\nHopes mounted for an agreement on cocoa buffer stock rules at an International Cocoa Organization, ICCO, council meeting which opened here today, delegates said. Both producer and consumer ICCO members said after the opening session that prospects for an agreement on the cocoa market support mechanism were improving. \"The chances are very good as of now of getting buffer stock rules by the end of next week,\" Ghanaian delegate and producer spokesm\n"
     ]
    }
   ],
   "source": [
    "print('target word:', target_word)\n",
    "# find our which columns of this matrix it belongs\n",
    "doc_idx = X_train_tfidf[:, vectorizer.vocabulary_.get(target_word)].nonzero()[0].tolist()\n",
    "\n",
    "# counts the number of documents that contain that target word\n",
    "print(len(doc_idx), 'documents found.')\n",
    "\n",
    "# then we gonna ransomly pick one of these documents actually contains that word. \n",
    "i = np.random.choice(len(doc_idx))\n",
    "print('Example:\\ndocument index:', doc_idx[i],)\n",
    "print('-----')\n",
    "print(corpus_train[doc_idx[i]][:500])"
   ]
  },
  {
   "source": [
    "#### What we want to notice is that ***tf-idf matrix is really sparse!***"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<9601x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 446797 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "source": [
    "#### Therefore, when we are doing topic modeling, i.e. extract the most important words from our corpus, we want to perform ***dimension reduction*** upon the matrix, and this step will involve ***SVD*** as follows."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Truncated SVD:\n",
    "- Project the tfidf vectors onto the first N principal components. \n",
    "- Though this is significantly fewer features than the original tfidf vector, they are stronger features, and the accuracy is higher.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Initiate TruncatedSVD, make a pipeline and train:\n",
    "\n",
    "- Dimensionality reduction using truncated SVD.\n",
    "- This transformer performs linear dimensionality reduction by means of truncated singular value decomposition (SVD). - Contrary to PCA, this estimator does not center the data before computing the singular value decomposition. This means it can work with sparse matrices efficiently.\n",
    "- **`n_componentsint`**: Desired dimensionality of output data.\n",
    "- **`algorithm`**: SVD solver to use. {‘arpack’, ‘randomized’}\n",
    "    - This estimator supports two algorithms: a fast randomized SVD solver due to Halko (2009), \n",
    "    - Or, “naive” algorithm that uses ARPACK wrapper in SciPy (scipy.sparse.linalg.svds) as an eigensolver on XX' or X'X, whichever is more efficient.\n",
    "- **`n_iter`**: int,default=5. Number of iterations\n",
    "    - For randomized SVD solver. Not used by ARPACK. \n",
    "    - The default is larger than the default in randomized_svd to handle sparse matrices that may have large slowly decaying spectrum.\n",
    "- **`random_state`**: default=None. Used during randomized svd. \n",
    "- **`tol`**: float, default=0, means machine precision. \n",
    "    - Tolerance for ARPACK. Ignored by randomized SVD solver.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Train TFIDF before SVD: (9601, 10000)\n",
      "\n",
      "Performing dimensionality reduction on TFIDF matrix using SVD...\n",
      "  done in 8.142sec\n",
      "\n",
      "Train TFIDF after SVD: (9601, 200)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "print(\"Train TFIDF before SVD:\", X_train_tfidf.shape)\n",
    "print(\"\\nPerforming dimensionality reduction on TFIDF matrix using SVD...\")\n",
    "t0 = time.time()\n",
    "\n",
    "# use ARPACK\n",
    "svd = TruncatedSVD(n_components=200, algorithm='arpack', tol=0)\n",
    "\n",
    "# making a LSA pipline by transforming X_train_tfidf, and ending with this matrix X_train_lsa\n",
    "lsa = make_pipeline(svd, \n",
    "#     Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    ")\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print(\"  done in %.3fsec\\n\" % (time.time() - t0))\n",
    "print(\"Train TFIDF after SVD:\",X_train_lsa.shape)"
   ]
  },
  {
   "source": [
    "#### What variance we are explaining when we are looking at 200 factors?"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Explained variance of the SVD step: 37%\n"
     ]
    }
   ],
   "source": [
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))"
   ]
  },
  {
   "source": [
    "Project on testing set:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   a300  a320  a330  a340   aa  aaa      aapl   ab  abandon  abandoned  ...  \\\n",
       "0   0.0   0.0   0.0   0.0  0.0  0.0  0.000000  0.0      0.0        0.0  ...   \n",
       "1   0.0   0.0   0.0   0.0  0.0  0.0  0.611085  0.0      0.0        0.0  ...   \n",
       "2   0.0   0.0   0.0   0.0  0.0  0.0  0.000000  0.0      0.0        0.0  ...   \n",
       "3   0.0   0.0   0.0   0.0  0.0  0.0  0.000000  0.0      0.0        0.0  ...   \n",
       "\n",
       "   zim  zimbabwe  zinc  ziyang  zoete  zone  zones  zorinsky  zuckerman  \\\n",
       "0  0.0       0.0   0.0     0.0    0.0   0.0    0.0       0.0        0.0   \n",
       "1  0.0       0.0   0.0     0.0    0.0   0.0    0.0       0.0        0.0   \n",
       "2  0.0       0.0   0.0     0.0    0.0   0.0    0.0       0.0        0.0   \n",
       "3  0.0       0.0   0.0     0.0    0.0   0.0    0.0       0.0        0.0   \n",
       "\n",
       "   zurich  \n",
       "0     0.0  \n",
       "1     0.0  \n",
       "2     0.0  \n",
       "3     0.0  \n",
       "\n",
       "[4 rows x 10000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a300</th>\n      <th>a320</th>\n      <th>a330</th>\n      <th>a340</th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aapl</th>\n      <th>ab</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>...</th>\n      <th>zim</th>\n      <th>zimbabwe</th>\n      <th>zinc</th>\n      <th>ziyang</th>\n      <th>zoete</th>\n      <th>zone</th>\n      <th>zones</th>\n      <th>zorinsky</th>\n      <th>zuckerman</th>\n      <th>zurich</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.611085</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4 rows × 10000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "X_test_tfidf = vectorizer.transform(test_sample)\n",
    "pd.DataFrame(X_test_tfidf.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test TFIDF before SVD: (4, 10000)\nTest TFIDF after SVD: (4, 200)\n"
     ]
    }
   ],
   "source": [
    "X_test_lsa = lsa.transform(X_test_tfidf)\n",
    "print(\"Test TFIDF before SVD:\",X_test_tfidf.shape)\n",
    "print(\"Test TFIDF after SVD:\",X_test_lsa.shape)"
   ]
  },
  {
   "source": [
    "### **our target: Find top 10 docs in corpus_train that are most similar to our ``test_sample`` of 4 strings.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Recommender Implement:\n",
    "\n",
    "Function: **``recommend(vec, X_model, X_corpus, lsa_instance)``**\n",
    "- projects any document vector `vec` onto a given ``X_model`` and returns ``doc_vec``, ``idx_top10``, ``sim_top10``, ``X_top10`` where\n",
    "    * `X_model`: {X_train_tfidf, X_train_lsa}\n",
    "    * `doc_vec`: the (sparse) vector of similarity scores of ``vec`` and members of ``X_model``. This vector should be size D × 1: 14459 number of documents in training set\n",
    "    * ``idx_top10``: the indices of the top-10 similarity scores\n",
    "    * ``sim_top10``: the top-10 similarity scores\n",
    "    * ``X_top10``: the top-10 corpus articles most similar to the input model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### TF-IDF:\n",
    "* Model_X: ``X_train_tfidf``\n",
    "* Test_vec: ``X_test_tfidf``\n",
    "### LSA:\n",
    "\n",
    "* Model_X: ``X_train_lsa``\n",
    "* Test_vec: ``X_test_lsa``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9601, 10000)\n(9601, 200)\n(4, 10000)\n(4, 200)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_tfidf))\n",
    "print(np.shape(X_train_lsa))\n",
    "print(np.shape(X_test_tfidf))\n",
    "print(np.shape(X_test_lsa))"
   ]
  },
  {
   "source": [
    "### Similarity Scores:\n",
    "* Assume we use **Cosine Similarity score on normalized vectors (unit)**\n",
    "$$ score(A,B) = \\frac{A\\cdot B}{||A||\\cdot ||B||} = A\\cdot B$$\n",
    "* To calculate/project any doc vector ``vec`` onto a given ``X_model``:\n",
    "    ```\n",
    "    score = vec.dot(X_model)\n",
    "\n",
    "    ```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(vec, X_model, X_corpus, lsa_instance):\n",
    "    '''\n",
    "    Args\n",
    "    ----------\n",
    "    vec: X_test_tfidf = trained_tfidf_vectorizer.transform(test_sample)\n",
    "    X_model: {X_train_tfidf, X_train_lsa}\n",
    "    X_corpus: corpus_train\n",
    "    lsa_instance: lsa trained above\n",
    "    \n",
    "    Returns\n",
    "    ---------\n",
    "    doc_vec: the (sparse) vector of similarity scores of vec and members of X_model. Dx1\n",
    "    idx_top10: the indices of the top-10 similarity scores\n",
    "    sim_top10: the top-10 similarity scores\n",
    "    X_top10: the top-10 corpus articles most similar to the input model\n",
    "\n",
    "    '''\n",
    "    X_train_tfidf_array = X_model[0].toarray()\n",
    "    X_train_lsa_array = X_model[1]\n",
    "    \n",
    "    X_test_tfidf_array = vec.toarray()\n",
    "    X_test_lsa_array = lsa_instance.transform(vec)\n",
    "\n",
    "    ########## tfidf\n",
    "    # score\n",
    "    score_tfidf = X_test_tfidf_array.dot(X_train_tfidf_array.T)\n",
    "    score_tfidf_vec = score_tfidf.sum(0)\n",
    "    df1 = pd.DataFrame(score_tfidf_vec, columns=['score tfidf'])\n",
    "    # top10 index\n",
    "    res1 = df1.sort_values(by='score tfidf',ascending=False).head(10)\n",
    "    idx1 = res1.index.values\n",
    "    # top10 values\n",
    "    s1 = res1['score tfidf'].values\n",
    "    # top10 articles\n",
    "    art1 = []\n",
    "    for i in idx1:\n",
    "        art1.append(X_corpus[i][:500])\n",
    "    \n",
    "    ########## LSA\n",
    "    # score\n",
    "    score_lsa = X_test_lsa_array.dot(X_train_lsa_array.T)\n",
    "    score_lsa_vec = score_lsa.sum(0)\n",
    "    df2 = pd.DataFrame(score_lsa_vec, columns=['score lsa'])\n",
    "\n",
    "    # top10 index\n",
    "    res2 = df2.sort_values(by='score lsa',ascending=False).head(10)\n",
    "    idx2 = res2.index.values\n",
    "\n",
    "    # top10 values\n",
    "    s2 = res2['score lsa'].values\n",
    "\n",
    "    # top10 articles\n",
    "    art2 = []\n",
    "    for i in idx2:\n",
    "        art2.append(X_corpus[i][:500])\n",
    "\n",
    "    doc_vec = (df1, df2)\n",
    "    idx_top10 = (idx1, idx2)\n",
    "    sim_top10 = (s1, s2)\n",
    "    X_top10 = (art1, art2)\n",
    "    return doc_vec, idx_top10, sim_top10, X_top10"
   ]
  },
  {
   "source": [
    "### Run ``recommend()`` function for the target ``test_sample`` vectors? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['Jabberwocky',\n",
       " 'buy MSFT sell AAPL hold Brent',\n",
       " 'bullish stocks',\n",
       " 'Some random forests produce deterministic losses']"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "test_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = X_test_tfidf\n",
    "X_model = [X_train_tfidf, X_train_lsa]\n",
    "corpus_train = corpus_train\n",
    "lsa = lsa\n",
    "doc_vec, idx_top10, sim_top10, X_top10 =  recommend(vec, X_model, corpus_train, lsa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSA: similarity scores between test_sample and each article in corpus\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   score lsa\n",
       "0   0.007706\n",
       "1   0.017369\n",
       "2  -0.005140\n",
       "3   0.044632\n",
       "4   0.009103\n",
       "5   0.003794\n",
       "6  -0.004190\n",
       "7  -0.004525\n",
       "8   0.002533\n",
       "9   0.007176"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score lsa</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.007706</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.017369</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>-0.005140</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.044632</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.009103</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.003794</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>-0.004190</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>-0.004525</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.002533</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.007176</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "print('LSA: similarity scores between test_sample and each article in corpus')\n",
    "pd.DataFrame(doc_vec[1]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSA: indices of the top 10 articles with highest scores\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "             similarity_score\n",
       "article_idx                  \n",
       "1682                 0.141920\n",
       "3751                 0.140121\n",
       "1687                 0.139713\n",
       "3747                 0.138814\n",
       "9219                 0.138448\n",
       "1190                 0.138330\n",
       "8659                 0.133026\n",
       "6013                 0.133026\n",
       "1202                 0.123061\n",
       "6020                 0.122553"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>similarity_score</th>\n    </tr>\n    <tr>\n      <th>article_idx</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1682</th>\n      <td>0.141920</td>\n    </tr>\n    <tr>\n      <th>3751</th>\n      <td>0.140121</td>\n    </tr>\n    <tr>\n      <th>1687</th>\n      <td>0.139713</td>\n    </tr>\n    <tr>\n      <th>3747</th>\n      <td>0.138814</td>\n    </tr>\n    <tr>\n      <th>9219</th>\n      <td>0.138448</td>\n    </tr>\n    <tr>\n      <th>1190</th>\n      <td>0.138330</td>\n    </tr>\n    <tr>\n      <th>8659</th>\n      <td>0.133026</td>\n    </tr>\n    <tr>\n      <th>6013</th>\n      <td>0.133026</td>\n    </tr>\n    <tr>\n      <th>1202</th>\n      <td>0.123061</td>\n    </tr>\n    <tr>\n      <th>6020</th>\n      <td>0.122553</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "print('LSA: indices of the top 10 articles with highest scores')\n",
    "pd.DataFrame({'article_idx':idx_top10[1], 'similarity_score': sim_top10[1]}).set_index('article_idx')"
   ]
  },
  {
   "source": [
    "### Make 10 recommendations using LSA:\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "LSA: Top 10 articals\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 3.4 MLN BBLS, GASOLINE OFF 100,000, CRUDE UP 3.2 MLN\n\n\n\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 8.8 mln barrels in the week ended March six to 119.6 mln barrels, the Energy Information Administration (EIA) said. In its weekly petroleum status report, the Department of Energy agency said gasoline stocks were off 500,000 barrels in the week to 251.0 mln barrels and refinery crude oil stocks fell 1.2 mln barrels to 331.8 mln. The EIA said residual fuel stocks fell 1.5 mln barrels to 36.4 mln ba\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 3.4 mln barrels in the week ended Feb 27 to 128.4 mln barrels, the Energy Information Administration (EIA) said. In its weekly petroleum status report, the Department of Energy agency said gasoline stocks were off 100,000 barrels in the week to 251.5 mln barrels and refinery crude oil stocks were up 3.2 mln barrels to 333.0 mln. The EIA said residual fuel stocks fell 2.2 mln barrels to 37.9 mln ba\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 8.8 MLN, GASOLINE OFF 500,000, CRUDE OFF 1.2 MLN\n\n\n\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 3.2 MLN BBLS, GASOLINE UP 2.2 MLN, CRUDE UP 7.5 MLN\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 4.4 MLN BBLS, GASOLINE OFF 30,000, CRUDE UP 700,000\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 4.07 MLN BBLS, GASOLINE OFF 2.69 MLN, CRUDE UP 8.53 MLN\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 7.35 MLN BBLS, GASOLINE OFF 2.89 MLN, CRUDE OFF 4.39 MLN \n\n\n\n\n*****\nAPI SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 4.4 mln barrels in the week ended Feb 27 to 127.10 mln barrels from 131.50 mln the previous week, the American Petroleum Institute (API) said. In its weekly statistical bulletin, the oil industry trade group said gasoline stocks fell 30,000 barrels to 252.92 mln barrels from a revised 252.95 mln, while crude oil stocks rose 700,000 barrels to 329.38 mln from a revised 328.68 mln. It said residual \n\n*****\nAPI SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 7.35 mln barrels in the week ended March 13 to 112.74 mln barrels from a revised 120.09 mln the previous week, the American Petroleum Institute (API) said. In its weekly statistical bulletin, the oil industry trade group said gasoline stocks fell 2.89 mln barrels to 248.44 mln barrels from a revised 251.33 mln, and crude oil stocks dropped 4.39 mln barrels to 325.13 mln from a revised 329.52 mln. \n"
     ]
    }
   ],
   "source": [
    "print('LSA: Top 10 articals')\n",
    "for t in X_top10[1]:\n",
    "    print('\\n'+'*'*5)\n",
    "    print(t)"
   ]
  },
  {
   "source": [
    "###Furthermore, let's compare these LSA results with the articles recommended by TFIDF only:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TFIDF: Top 10 articals\n\n*****\nANALYST REITERATES BUY ON SOME DRUG STOCKS\n\nMerrill Lynch and Co analyst Richard Vietor said he reiterated a buy recommendation on several drug stocks today. The stocks were Bristol-Myers Co BMY>, which rose 2-1/4 to 101, Schering-Plough Corp SGP> 2-7/8 to 97 and Syntex Corp SYN> 1-3/8 to 82. Vietor described these stocks as a \"middle group\" of performers. Vietor said the prices of these stocks, \"look pretty cheap relative to the leading performers in the drug group, such as Upjohn Co UPJ>, Merc\n\n*****\nSUBROTO SEES OIL MARKET CONTINUING BULLISH\n\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a press conference in Jakarta at the end of a two-day meeting of South-East Asian Energy Ministers that he saw prices stabilizing around 18 dlrs a barrel. \"The sentiment in the market is bullish and I think it will continue that way as demand will go up in the third or fourth quarters,\" Subroto said. Aske\n\n*****\nSUBROTO SEES OIL MARKET CONTINUING BULLISH\n\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a press conference in Jakarta at the end of a two-day meeting of South-East Asian Energy Ministers that he saw prices stabilizing around 18 dlrs a barrel. \"The sentiment in the market is bullish and I think it will continue that way as demand will go up in the third or fourth quarters,\" Subroto said. Aske\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 3.4 mln barrels in the week ended Feb 27 to 128.4 mln barrels, the Energy Information Administration (EIA) said. In its weekly petroleum status report, the Department of Energy agency said gasoline stocks were off 100,000 barrels in the week to 251.5 mln barrels and refinery crude oil stocks were up 3.2 mln barrels to 333.0 mln. The EIA said residual fuel stocks fell 2.2 mln barrels to 37.9 mln ba\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 8.8 mln barrels in the week ended March six to 119.6 mln barrels, the Energy Information Administration (EIA) said. In its weekly petroleum status report, the Department of Energy agency said gasoline stocks were off 500,000 barrels in the week to 251.0 mln barrels and refinery crude oil stocks fell 1.2 mln barrels to 331.8 mln. The EIA said residual fuel stocks fell 1.5 mln barrels to 36.4 mln ba\n\n*****\nWALL STREET SURVIVES TRIPLE EXPIRATIONS\n\nThe four-times-a-year \"triple witching hour\" did not jolt Wall Street as much as it has in the past. Market averages finished sharply higher as stock index futures, index options and options on individual stocks expired simultaenously. Some analysts warned part of the gain may be retraced next week. But there were signs Wall Street is getting used to the phenomeon which causes a huge burst of activity in the final minutes. Officials of the New York Stock \n\n*****\nPARIS TO ADD THREE STOCKS TO CONTINUOUS QUOTATION\n\nThe Paris Bourse will add a further three stocks to its computerised continuous trading system from March 24, bringing the total number of continuously traded stocks to 54, the Stockbrokers' Association said. The shares are Bazar de l'Hotel de Ville> (BHV), Dollfus Mieg et Cie> (DMC) and Maisons Phenix>. The Bourse has said it hopes to have at least 100 companies quoted continuously by the end of this year. Reuter \n\n*****\nDRAWDOWN SEEN IN U.S. DISTILLATE STOCKS\n\nTonight's American Petroleum Institute oil inventory report is expected to show another drawdown in distillate stocks of between two and 7.5 mln barrels for the week ending March 20, oil analysts and traders said. They said they expect gasoline inventories to be depleted by about one to four mln barrels. Analysts were divided on the crude stocks. Some saw stocks unchanged to as much as three mln barrels higher. Others said stocks could be down one to five\n\n*****\nTALKING POINT/TOBACCO STOCKS\n\nStocks of tobacco companies rose sharply as investors grew more confident that an excise tax would not be imposed on tobacco, traders and analysts said. They also said the stocks are relatively inexpensive since fear of the tax and of pending litigation regarding warning labels for tobacco products have kept many investors away recently. Philip Morris Cos MO> rose 2-5/8 to 87-3/4, RJR Nabisco RJR> 1-1/4 to 57-1/2 and U.S. Tobacco UBO> 7/8 to 27. \"The near-term activ\n\n*****\nTROPICAL FOREST DEATH COULD SPARK NEW DEBT CRISIS\n\nThe death of the world's tropical rain forests could trigger a new debt crisis and social and biological disasters, scientists and ecologists involved with the International Tropical Timber Organisation (ITTO) said. At stake is the ability of developing nations, including Brazil, Mexico and the Philippines, to service their debts and the loss of trade worth hundreds of billions of dollars in important sectors such as agriculture and pharmaceutic\n"
     ]
    }
   ],
   "source": [
    "print('TFIDF: Top 10 articals')\n",
    "for t in X_top10[0]:\n",
    "    print('\\n'+'*'*5)\n",
    "    print(t)"
   ]
  },
  {
   "source": [
    "### Compared to TF-IDF, LSA improves the recommendation giving more relevent articles!!!!"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-------\n",
    "## Recommender For Jokes\n",
    "\n",
    "Data/Corpus:\n",
    "- 200K English plaintext jokes: https://github.com/taivop/jokedataset. \n",
    "\n",
    "Goal:\n",
    "- Construct a recommender system that can find similar jokes. \n",
    "- Give examples of good and bad recommendations. \n",
    "- Provide a list of suggestions of how one could improve upon this recommender."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Import data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total number of jokes: 208345\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "url1=\"https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json\"\n",
    "url2='https://raw.githubusercontent.com/taivop/joke-dataset/master/stupidstuff.json'\n",
    "url3='https://raw.githubusercontent.com/taivop/joke-dataset/master/wocka.json'\n",
    "\n",
    "urls=[url1,url2,url3]\n",
    "jokes = []\n",
    "for idx, url in enumerate(urls):\n",
    "    r=requests.get(url)\n",
    "    t=json.loads(r.content)\n",
    "    for i in range(len(t)):\n",
    "        if idx == 0:\n",
    "            jokes.append(t[i]['title']+'\\n\\n'+t[i]['body'])\n",
    "        elif idx ==1:\n",
    "            jokes.append(t[i]['category']+'\\n\\n'+t[i]['body'])\n",
    "        else:\n",
    "            jokes.append(t[i]['title']+'\\n\\n'+t[i]['body'] +'\\n\\n'+t[i]['category'])\n",
    "print('Total number of jokes:',len(jokes))"
   ]
  },
  {
   "source": [
    "Let's read some:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n***************\n I hate how you cant even say black paint anymore\n\nNow I have to say \"Leroy can you please paint the fence?\" \n\n\n***************\n What's the difference between a Jew in Nazi Germany and pizza ?\n\nPizza doesn't scream when you put it in the oven .\n\nI'm so sorry. \n\n\n***************\n I recently went to America....\n\n...and being there really helped me learn about American culture. So I visited a shop and as I was leaving, the Shopkeeper said \"Have a nice day!\" But I didn't so I sued him. \n\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate(jokes[:3]):\n",
    "    print('\\n'+'*'*15+'\\n',j,'\\n')"
   ]
  },
  {
   "source": [
    "### Setup:\n",
    "* ``N``: number of recommendations we want to make\n",
    "* ``joke_universe``: our database for searching\n",
    "* ``joke_target``: we want some joke recommendations that are similar to this one"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "We have 208344 jokes to recommend from.\nWe want to find 10 jokes that are the most similar to our \ntarget joke below:\n\n***************\n... And We Wonder Why Everyone Hates Us\n\nCustomer: \"Are you Hispanic?\"\n\nMe: \"No.\"\n\nCustomer: \"Middle Eastern?\"\n\nMe: \"No.\"\n\nCustomer: \"Egyptian?\"\n\nMe: \"No.\"\n\nCustomer: \"What are you?\"\n\nMe: \"Chinese.\"\n\n(customer puts on offended face)\n\nCustomer: \"I don't appreciate you treating me like I'm dumb.\"\n\nMe: \"Excuse me? I'm being honest.\"\n\nCustomer: \"NO CHINESE PERSON WOULD EVER HAVE EYES AS BIG AS YOURS!!!\"\n\nMe: *mouth wide open*\n\nInsults\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "joke_universe = jokes[:-1]\n",
    "joke_target = jokes[-1:]\n",
    "print(f'We have {len(joke_universe)} jokes to recommend from.\\nWe want to find {N} jokes that are the most similar to our \\ntarget joke below:\\n')\n",
    "print('*'*15)\n",
    "print(joke_target[0])"
   ]
  },
  {
   "source": [
    "### Recommender System:\n",
    "- Perform LSA (TruncatedSVD on TF-IDF) on joke universe\n",
    "- Project on testingset: target joke "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=3000,\n",
    "    min_df=2,\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "    )\n",
    "\n",
    "joke_X_train_tfidf = joke_tfidf_vectorizer.fit_transform(joke_universe)\n",
    "joke_features_tfidf = joke_tfidf_vectorizer.get_feature_names()\n",
    "joke_X_test_tfidf = joke_tfidf_vectorizer.transform(joke_target)\n",
    "joke_X_train_tfidf_array = joke_X_train_tfidf.toarray()\n",
    "joke_X_test_tfidf_array = joke_X_test_tfidf.toarray()\n",
    "\n",
    "joke_score_tfidf = joke_X_test_tfidf_array.dot(joke_X_train_tfidf_array.T)\n",
    "joke_score_tfidf_vec = joke_score_tfidf.sum(0)\n",
    "\n",
    "joke_df1 = pd.DataFrame(joke_score_tfidf_vec, columns=['score tfidf'])\n",
    "joke_res1 = joke_df1.sort_values(by='score tfidf',ascending=False).head(10) # top 10\n",
    "joke_idx1 = joke_res1.index.values # top 10 idx\n",
    "joke_s1 = joke_res1['score tfidf'].values # top 10 scores\n",
    "joke_art1 = [] # top 10 jokes\n",
    "for i in joke_idx1:\n",
    "    joke_art1.append(joke_universe[i])"
   ]
  },
  {
   "source": [
    "Visualize TD-IDF recommendations:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n***************\n Paging Leonidas To The Front Desk\n\nCustomer: \"Look! My friend told me I could get this type of hammer at your store! Now go get it for me!\"\n\nCashier: \"Sir, I already told you... we don't have ANY hammers back here that aren't already stocked on the shelves.\"\n\nCustomer: \"LOOK HERE. F**K YOU! I KNOW YOU'RE TRYING TO SAVE MONEY BY SWITCHING OUT YOUR STOCKS! GET ME THIS HAMMER!\"\n\n(At this point, I come to the front of the store, overhearing what's going on; note that I'm the manager.)\n\nMe: \"Is there a problem?\"\n\nCustomer: \"Yes sir! Your employee here is not doing what I tell her to!\"\n\nMe: \"Well, you need to calm down and understand that we don't have what you're looking for. So maybe you should go back to shelves and checkâ\"\n\nCustomer: \"F**K THAT!!! IT'S NOT THERE, OKAY?! YOU NEED TO F**KING GET ME WHAT I ASK FOR!\"\n\nMe: \"That's it. Get out of my store.\"\n\nCustomer: \"What? NO!\"\n\nMe: \"Sir, get out, or I have to take you out.\"\n\nCustomer: \"Then do it!\"\n\n(I go around the counter and approach the customer. I yank him by his collar & drag him to the door.)\n\nMe: \"Now, then... you wanna apologize and maybe come back in?\"\n\nCustomer: \"No! I just want my hammer! God, what is this madness?!\"\n\nMe: *puts the customer down*\n\nCustomer: *confused* \"... What is it?\"\n\n(I turn back to the cashier, who nods in approval. I then turn back to face the customer.)\n\nMe: \"Madness? THIS! IS! SPARTAAAAAAAAA!\" *kicks customer out of store and slams door*\n\nAt Work \n\n\n***************\n Barista asks a customer if they would like their coffee black\n\nCustomer replies \"what other colors do you have?\" \n\n\n***************\n As The Checkout Line Churns\n\n(I'm ringing up a customer and notice her last name is the same as mine. I have a very uncommon last name, so I made the mistake of mentioning this...)\n\nMe: \"Your last name is [name]? Mine, too. Wonder if we're related?\" *chuckle*\n\nCustomer: *very serious* \"What is your name?\"\n\nMe: \"Oh, I was joking, we're not related; almost all of my family lives up in New England.\"\n\nCustomer: *more serious* \"What is your name?\"\n\nMe: \"Uhhh...I'm noâ\"\n\nCustomer: \"Do you have a brother named [brother's name]?\"\n\nMe: \"Yes, actually...\"\n\nCustomer: \"Is your mother [mom's name]?\"\n\nMe: \"Uh, yeah...\"\n\nCustomer: \"And your father's name is [my estranged father's name]?\"\n\nMe: \"Well, he's my biological father, yes.\"\n\nCustomer: *sticks out hand* \"Nice to meet you, I'm your step-mother!\"\n\n(The entire line of about a dozen people behind her gasps, like they were watching a soap opera.)\n\nMe: \"Oh, God...please don't tell my father I work here.\"\n\nCustomer: \"You know why your father left your mother, right?\"\n\nMe: \"Uh...no?\"\n\nCustomer: \"Because she cheated on him with [my stepfather]!\"\n\n(The line behind her gasps again.)\n\nMe: \"Oh, okay...\"\n\nCustomer: \"You know, your father is very heartbroken about you. You've grown up to be such a beautiful young woman. You should call him and talk to him just so he can see how you're doing.\"\n\nMe: \"Actually, we don'tâ\"\n\nCustomer: \"You and I need to go out for coffee sometime. I have a lot of stories to tell you.\"\n\nMe: \"Okay, wellâ\"\n\nCustomer: \"I promise, I'm not an evil stepmother. Well, I'll see you later, sweetie!\" *bounces out the front door*\n\nMe: *speechless*\n\nNext customer: \"Sweetie, are you okay?\"\n\nMe: *still speechless*\n\nNext customer: \"Why don't you take a break? We don't mind waiting.\"\n\nEntire line: \"No! Go take a break!\"\n\nMe, to my boss: \"Hey, I'm taking a break. I'll be back inâ\"\n\nBoss: \"For God's sake, go home! I'll see you on Monday.\"\n\nMen / Women \n\n\n***************\n Customer Service\n\nA customer approaches the customer service desk and looks at the representative's name-badge.\n\nThe customer points to the badge and says, \"How do you say?\"\n\nThe representative tilts his head and says, \"...Pat.\"\n\nThe customer nods and says, \"Pat. You say name for me?\"  \nThe customer pulls a paper out of his pocket.\n\nPat looks at the name and exclaims, \"Jesus!\"\n\n\"Jesus, thanks Pat.\"\n \n\n\n***************\n Store Policy\n\nCUSTOMER: I'd like to buy some dog food.\nCHECKOUT: Do you have a dog?\nCUSTOMER: Yes.\nCHECKOUT: Where is he?\nCUSTOMER: He's at home.\nCHECKOUT LADY: I'm sorry, I can't sell this dog food to you unless I see the dog. Store policy.\n\nThe next day, the customer returns.\nCUSTOMER: I'd like to buy some cat food.\nCHECKOUT: Do you have a cat?\nCUSTOMER: Yes.\nCHECKOUT: Well...where is he?\nCUSTOMER: He's at home!\nCHECKOUT: Sorry, I can't sell this cat food to you unless I see your cat.\n\nThe next day the customer returns.\nCHECKOUT: What's in the sack?\nCUSTOMER: Put your hand inside.\nCHECKOUT: Hmmm...It's warm and moist! What is it?\nCUSTOMER: I would like to buy some toilet paper. \n\n\n***************\n A customer asked me for a good reliable printer...\n\n \n\n\n***************\n Comcast's Customer Service\n\nba dum tsss \n\n\n***************\n Valve improving their customer service.\n\n \n\n\n***************\n What did the server say when the customer requested something they didn't have?\n\n404 \n\n\n***************\n Cars\n\nWhat if people bought cars like they buy\nComputers?\n\nThe car companies don't have help lines\nfor people who don't know how to drive,\nbecause people don't buy cars like they\nbuy computers, imagine if they did.....\n\nHelpline: General Motors Helpline, how can I help\nyou?\n\nCustomer: I got in my car and closed the door and\nnothing happened!\n\nHelpline: Did you put the key in the ignition slot\nand turn it?\n\nCustomer: What's an ignition?\n\nHelpline: It's a starter motor that draws current\nfrom your battery and turns over the engine.\n\nCustomer: Ignition? Motor? Battery? Engine? How\ncome I have to know all these technical terms to\nuse my car.\n\nHelpline: Toyota Helpline, how can I help you?\n\nCustomer: My car ran fine for a week and now it\nwon't go anywhere!\n\nHelpline: Is the gas tank empty?\n\nCustomer: Huh?  How do I know?\n\nHelpline: There's a little gauge on the front\npanel with a needle and markings of 'E' and 'F'.\nWhere is the needle pointing?\n\nCustomer: It's pointing to 'E'. What does that\nmean?\n\nHelpline: It means you have to visit a gasoline\nvendor and purchase some more gasoline. You can\ninstall it yourself or pay the vendor to install\nit for you.\n\nCustomer: What? I paid $18,000 for this car!\nAnd you're telling me I need to keep buying more\ncomponents? This is outrageous! I want a car that\ncomes with everything built in!\n\nHelpline: Ford Helpline, how can I help you?\n\nCustomer: Your cars suck!\n\nHelpline: What's wrong?\n\nCustomer: It crashed, that's what's wrong!\n\nHelpline: What were you doing?\n\nCustomer: Well I wanted to go faster, so I pushed\nthe accelerator pedal all the way to the floor, it\nworked for a while and then it when off the road\nat a corner and crashed and it won't start now!\n\nHelpline: It's your responsibility if you misuse\nthe product. What do you expect us to do about it?\n\nCustomer: I expect you to send me one of the\nlatest versions that doesn't crash!\n\nHelpline: BMW Helpline, how can I help you?\n\nCustomer: Hi, I just bought my first car, and I\nchose your car because it has automatic\ntransmission, cruise control, power steering,\npower brakes, power door locks, power seats,\npower..\n\nHelpline:  Well,.. thanks for buying one of our\ntop of line cars. So how can I help you?\n\nCustomer: Well, how do I work it?\n\nHelpline: Do you know how to drive?\n\nCustomer: Do I know how to what?\n\nHelpline: Do you know how to drive?\n\nCustomer: Look, I'm not a mechanic. I'm not even\nvery technical.  I just want to go places in my\nnew car!\n\nOther / Misc \n\n"
     ]
    }
   ],
   "source": [
    "for j in joke_art1:\n",
    "    print('\\n'+'*'*15+'\\n',j,'\\n')"
   ]
  },
  {
   "source": [
    "#### Truncated SVD/LSA:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "joke_svd = TruncatedSVD(n_components=200,random_state=42,algorithm='arpack')\n",
    "joke_lsa = make_pipeline(joke_svd) \n",
    "#   Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    "\n",
    "joke_X_train_lsa = joke_lsa.fit_transform(joke_X_train_tfidf)\n",
    "joke_X_train_lsa_array = joke_X_train_lsa\n",
    "\n",
    "joke_X_test_lsa = joke_lsa.transform(joke_X_test_tfidf)\n",
    "joke_score_lsa = joke_X_test_lsa.dot(joke_X_train_lsa_array.T)\n",
    "joke_score_lsa_vec = joke_score_lsa.sum(0)\n",
    "\n",
    "joke_df2 = pd.DataFrame(joke_score_lsa_vec, columns=['score lsa'])\n",
    "joke_res2 = joke_df2.sort_values(by='score lsa',ascending=False).head(10)\n",
    "joke_idx2 = joke_res2.index.values\n",
    "joke_s2 = joke_res2['score lsa'].values\n",
    "joke_art2 = []\n",
    "for i in joke_idx2:\n",
    "    joke_art2.append(joke_universe[i])"
   ]
  },
  {
   "source": [
    "Visualize LSA recommendations:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n***************\n what do you call a Chinese person with down syndrome?\n\nSom ting wong \n\n\n***************\n How do you blindfold a chinese person?\n\nWith dental floss \n\n\n***************\n What do you call a Chinese Millionaire?\n\nCha Ching \n\n\n***************\n What do you call a foreigner who is obsessed with Chinese culture?\n\nA zhuologist \n\n\n***************\n What do you call a Chinese Podiatrist?\n\nHee Lan To \n\n\n***************\n What part of your punctuality emancipates the Chinese?\n\nYour Ti\"ming\"! \n\n\n***************\n What do you call a Chinese millionaire?\n\nCha Ching \n\n\n***************\n What do Chinese lumberjacks do?\n\nChopsticks \n\n\n***************\n what do you call a chinese millionaire?\n\nCha-Ching \n\n\n***************\n What do you call a chinese millionaire?\n\nCha-Ching! \n\n"
     ]
    }
   ],
   "source": [
    "for j in joke_art2:\n",
    "    print('\\n'+'*'*15+'\\n',j,'\\n')"
   ]
  },
  {
   "source": [
    "### Conclusion:\n",
    "* Accidently, my joke target is kind of about sensitive topic under ``insult`` category I guess. Sorry about that.\n",
    "* But if we assume that I want to find out all of those most impolite jokes, and want to delete them immediately, which models give me better delete recommendations?\n",
    "* According to what I printed above, obviously ``SVD/LSA`` model directs me to a better way.\n",
    "----\n",
    "* My **target joke** is:\n",
    "\n",
    "    ... And We Wonder Why Everyone Hates Us\n",
    "    \n",
    "    Customer: \"Are you Hispanic?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"Middle Eastern?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"Egyptian?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"What are you?\"\n",
    "    \n",
    "    Me: \"Chinese.\"\n",
    "    \n",
    "    (customer puts on offended face)\n",
    "    \n",
    "    Customer: \"I don't appreciate you treating me like I'm dumb.\"\n",
    "    \n",
    "    Me: \"Excuse me? I'm being honest.\"\n",
    "    \n",
    "    Customer: \"NO CHINESE PERSON WOULD EVER HAVE EYES AS BIG AS YOURS!!!\"\n",
    "    \n",
    "    Me: *mouth wide open*\n",
    "    \n",
    "    Insults\n",
    "\n",
    "\n",
    "* A **bad recommendation** from ``TF-IDF``:\n",
    "\n",
    "    -------This is how a joke index  173384  looks like:-------\n",
    "\n",
    "     A customer asked me for a good reliable printer...\n",
    "\n",
    "\n",
    "* A **good recommendation** from ``SVD/LSA``:\n",
    "\n",
    "    -------This is how a joke index  117910  looks like:-------\n",
    "\n",
    "     What do Chinese lumberjacks do?\n",
    "\n",
    "     Chopsticks\n",
    "\n",
    "### List of suggestions of how one could improve upon this recommender：\n",
    "* We already see in my data importing cell, there are three different data source and hence three different json dictionary structures. When doing this recommender, I already considered put 'body','title','category' into joke_universe if there are. But notice that, we still have more options to consider, such as 'id','rating'.We can in one way think of adding these two options in our joke universe.\n",
    "* Another way to improve may be to do our own classification first. Since we had some 'category' field from stupidstuff.json and wocka.json, but not from reddit_jokes.json. So, we can import all fields and do a classification to re-classify all of our jokes in to some self-defined categories (more detailed). Taking this new category into consideration when training the model may improve the performance of our recommender.\n",
    "* Besides, according to LSA results, we can see some duplicated jokes, which means we need clean the dataset first. i.e. delete repeated ones.\n",
    "* One trivial but maybe important way is to try different parameters for our initial model instances/objects: ``TfidfVectorizer``, ``TruncatedSVD``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-------------------------------------------------\n",
    "## Appendix: More about NLP\n",
    "\n",
    "NLP transforms human language into machine-usable code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Processing Techs & Terms\n",
    "\n",
    "- Tokenization: splitting text into individual words (tokens)\n",
    "- Lemmatization: reduces words into its base form based on dictionary definition (am, is, are -> be)\n",
    "- Stemming: reduces words to its base form without context (ended -> end)\n",
    "- Stop words: remove common and irrelevant words (the, is)\n",
    "\n",
    "`n-gram`: predicts the next term in a sequence of n terms based on Markov Chains (stochastic and memoryless process that predicts future events based only on the current state)\n",
    "\n",
    "`bag-of-words`: represents text using word frequencies without context or order\n",
    "\n",
    "`tf-idf`: term-frequency inverse-document-frequency. Measures word importance for a document in a collection of documents (corpus), by multiplying the term frequency (occurences of a term in a document) with the inverse document frequency, in which way it penalizes common terms across a corpus.\n",
    "\n",
    "`Cosine Similarity`: meansure the similarity between vectors, calculated as $$\\text{cos}(\\theta) = \\frac{A\\cdot B}{\\text{||}A\\text{||}\\text{ ||}B\\text{||}}$$"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Applications & Models\n",
    "\n",
    "#### Word Embedding\n",
    "Maps words and phrases to numerical vectors\n",
    "\n",
    "- **word2vec**: \n",
    "    - trains iterativesly over local word context windows, places similar words close together, and embeds sub-relationships directly into vectors. \n",
    "    - E.g. *king - man + woman = queen*\n",
    "    - Two approaches:\n",
    "        - Continuous bag-of-words (CBOW), predicting the word given its context\n",
    "        - skip-gram, predicting the context given a word\n",
    "\n",
    "- **GloVe**:\n",
    "    - combines both global and local word co-occurance data to learn word similarity\n",
    "\n",
    "- **BERT**:\n",
    "    - accounts for word order and trains on subwords, and unlike word2vec and GloVe, BERT outputs different vectors for different uses of words\n",
    "    - E.g. *cell phone VS blood cell*"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Sentiment Analysis\n",
    "Extracts the attitudes and emotionis from text.\n",
    "\n",
    "- **Polarity**: measures positive, negative, or neutral opinions\n",
    "    - Valence shifters: capture amplifiers or negators such as \"***really*** fun\" or \"*hardly* fun\"\n",
    "\n",
    "- **Sentiment**: measures the emotional states such as happy or sad\n",
    "- **Subject-Object Identification**: classifies sentences as either S or O\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Topic Modeling\n",
    "Captures the underlying themes taht appear in documents.\n",
    "\n",
    "- **Latent Dirichlet Allocation (LDA)**: generate k topics by first assigning each word to a random topic, then iteratively updating assignments based on parameters $\\alpha$, the mix of topics per document, and $\\beta$, the distribution of words per topic.\n",
    "\n",
    "- **Latent Semantic Analysis (LSA)**: identifies patterns using TF-IDF scores and reduces data to k dimensions through SVD. In other words, given a corpus of articles, we want to create a term-document-type of matrix, for which we can do SVD analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mysixenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3d88cacd8b745d4fbed2830424fce0451ea91f459b833c1d75c3abe001a2f0c4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Natural Language Processing: Latent Semantic Analysis\n",
    "\n",
    "Background: \n",
    "\n",
    "In NLP, one important application is about Topic Modeling, with which we try to capture the underlying themes that appear in a group of documents. Basically, we have two approaches:\n",
    "\n",
    "- Latent Dirichlet Allocation (LDA) -  generate k topics by first assigning each word to a random topic, then iteratively updating assignments based on parameters $\\alpha$, the mix of topics per document, and $\\beta$, the distribution of words per topic.\n",
    "\n",
    "- Latent Semantic Analysis (LSA) - identifies patterns using TF-IDF scores and reduces data to k dimensions through SVD. In other words, given a corpus of articles, we want to create a term-document-type of matrix, for which we can do SVD analysis.\n",
    "\n",
    "### In this project, we want to empoly ***LSA***.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse.csr as csr\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "# I. Representing a corpus of documents as a matrix\n",
    "## Bag of Words matrix\n",
    "\n",
    "Following the <a href=\"http://scikit-learn.org/stable/modules/feature_extraction.html\">Sklearn Feature-extraction documentation page</a>\n",
    "\n",
    "- we start with a given **corpus** of $D$ documents.\n",
    "- we preprocess each document and convert it into a list of terms (features)\n",
    "    - by lowercasing first\n",
    "    - accepting only word patterns (defined via regex)\n",
    "- then we form the $CV$ Count-Vectorizer term-frequency matrix defined as:\n",
    "\n",
    "$$\n",
    "\\text{tf}(t, d)\\equiv{CF}_{d,t} = \\text{# times term }t\\text{ occurs in document }d\n",
    "$$\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=None, vocabulary=None)"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# preprocessing\n",
    "# a default type of pattern for stop words\n",
    "tpatterns = [\n",
    "    '(?u)\\\\b\\\\w\\\\w+\\\\b', #default tpatterns[0]\n",
    "    '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b', #tpatterns[1]\n",
    "    '\\\\w',#tpatterns[2]\n",
    "    '\\\\w+',#tpatterns[3]\n",
    "    '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b|\\\\b[0-9]\\\\b'#tpatterns[4]\n",
    "]\n",
    "\n",
    "# instantiate a contvectorizer, which just does the TF transformation\n",
    "vectorizer = CountVectorizer(token_pattern=tpatterns[0])\n",
    "vectorizer"
   ]
  },
  {
   "source": [
    "### Reference for ***regular expression***\n",
    "https://www.ntu.edu.sg/home/ehchua/programming/howto/Regexe.html"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "# start with four documents\n",
    "corpus = [\n",
    "    'This is the the first first document abra.',\n",
    "    'This is the second second document cadabra.',\n",
    "    'And the third one 3.',\n",
    "    'Is this the first document 4?',\n",
    "]\n",
    "# list of string\n",
    "# map corpus onto a matrix 4x11: 4 documents, 11 unique words/terms\n",
    "X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "X_corpus_docterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 2, 1, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "X_corpus_docterm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['abra',\n",
       " 'and',\n",
       " 'cadabra',\n",
       " 'document',\n",
       " 'first',\n",
       " 'is',\n",
       " 'one',\n",
       " 'second',\n",
       " 'the',\n",
       " 'third',\n",
       " 'this']"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# note the effect of modifying the token_pattern above....\n",
    "features = vectorizer.get_feature_names() \n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   abra  and  cadabra  document  first  is  one  second  the  third  this\n",
       "0     1    0        0         1      2   1    0       0    2      0     1\n",
       "1     0    0        1         1      0   1    0       2    1      0     1\n",
       "2     0    1        0         0      0   0    1       0    1      1     0\n",
       "3     0    0        0         1      1   1    0       0    1      0     1"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abra</th>\n      <th>and</th>\n      <th>cadabra</th>\n      <th>document</th>\n      <th>first</th>\n      <th>is</th>\n      <th>one</th>\n      <th>second</th>\n      <th>the</th>\n      <th>third</th>\n      <th>this</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "pd.DataFrame(X_corpus_docterm.toarray(),columns = np.array(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(?u)\\b\\w\\w+\\b\n['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n(?u)\\b[a-zA-Z]\\w+\\b\n['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n\\w\n['3', '4', 'a', 'b', 'c', 'd', 'e', 'f', 'h', 'i', 'm', 'n', 'o', 'r', 's', 't', 'u']\n\n\n\\w+\n['3', '4', 'abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n(?u)\\b[a-zA-Z]\\w+\\b|\\b[0-9]\\b\n['3', '4', 'abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\n"
     ]
    }
   ],
   "source": [
    "corpus = [\n",
    "    'This is the the first first document abra.',\n",
    "    'This is the second second document cadabra.',\n",
    "    'And the third one 3.',\n",
    "    'Is this the first document 4?',\n",
    "]\n",
    "\n",
    "for tp in tpatterns:\n",
    "    vectorizer.token_pattern = tp\n",
    "    print(vectorizer.token_pattern)\n",
    "    X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "    features = vectorizer.get_feature_names()\n",
    "    print(features)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "vectorizer.token_pattern = tpatterns[0] #back to the default\n",
    "X_corpus_docterm = vectorizer.fit_transform(corpus)\n",
    "features = vectorizer.get_feature_names()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4x11 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 21 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "X_corpus_docterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['This is the the first first document abra.',\n",
       " 'This is the second second document cadabra.',\n",
       " 'And the third one 3.',\n",
       " 'Is this the first document 4?']"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 1, 2, 1, 0, 0, 2, 0, 1],\n",
       "       [0, 0, 1, 1, 0, 1, 0, 2, 1, 0, 1],\n",
       "       [0, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "# This is our document-term matrix CV:\n",
    "CV = X_corpus_docterm.toarray()\n",
    "CV\n",
    "# the first document contains 'first' twice, 第一行第五个2"
   ]
  },
  {
   "source": [
    "## What if we want to vectorize a document in the test set?\n",
    "\n",
    "E.g. this could even be a document with words not even encountered before"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['abra', 'and', 'cadabra', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n\n\nrain\n[[0 0 0 0 0 0 0 0 0 0 0]]\n\n\nThe a yoghurt\n[[0 0 0 0 0 0 0 0 1 0 0]]\n\n\nthree two one hurray\n[[0 0 0 0 0 0 1 0 0 0 0]]\n\n\nand abra document is first cadabra\n[[1 1 1 1 1 1 0 0 0 0 0]]\n\n\none-third abra is the first cadabra and this document a second\n[[1 1 1 1 1 1 1 1 1 1 1]]\n\n\n[[0 0 0 0 0 0 0 0 0 0 0]\n [0 0 0 0 0 0 0 0 1 0 0]\n [0 0 0 0 0 0 1 0 0 0 0]\n [1 1 1 1 1 1 0 0 0 0 0]\n [1 1 1 1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# just call the .transform() method of the trained vectorizer\n",
    "\n",
    "# what is we see a term in test set\n",
    "# that has never appeared before in training set?\n",
    "def docs2vec(docs, vectorizer):\n",
    "    return vectorizer.transform(docs)\n",
    "\n",
    "docs_test = [\n",
    "    'rain',\n",
    "    'The a yoghurt',\n",
    "    'three two one hurray',\n",
    "    'and abra document is first cadabra',\n",
    "    'one-third abra is the first cadabra and this document a second',\n",
    "]\n",
    "\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "for doc in docs_test:\n",
    "    print('\\n')\n",
    "    print(doc)\n",
    "    print(docs2vec([doc], vectorizer).toarray())\n",
    "\n",
    "X_docterm_test = docs2vec(docs_test, vectorizer)\n",
    "print('\\n')\n",
    "print(X_docterm_test.toarray())"
   ]
  },
  {
   "source": [
    "### Problems with Term-Document matrix:\n",
    "\n",
    "- The document vectors are **not normalized**\n",
    "    - so can't really compare documents\n",
    "    \n",
    "- The document vectors contain many common english words containing **no information**\n",
    "    - ideally we want to remove those, e.g. 'the', 'is', etc\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Solution: TF-IDF vectorizer (see <a href=http://scikit-learn.org/stable/modules/feature_extraction.html> The TF-idf section in the Scikit-Learn feature extraction manual</a>)\n",
    "\n",
    "Instead, let's consider the following matrix\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\text{tf-idf}(t,d) &\\equiv{\\text{tf}}(t,d)\\times\\text{idf}(t)\\\\\n",
    "\\text{idf}(t)&\\equiv\\log\\frac{1+n_d}{1+\\text{df}(d,t)} + 1\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "- $\\text{df}(d,t)$ is the number of documents containing feature $t$\n",
    "- the rows of the tf-idf matrix are normalized to have unit norm (either $L_1$ or $L_2$)\n",
    "    - this way we can compare documents by the norm of their doc2vec overlaps\n",
    "    \n",
    "Let's see this in practice"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "blah abra cadabra\nblah abra cadabra\nblah abra \nblah abra \nblah abra \nblah abra \nblah abra \nblah abra \nblah abra \nblah abra \n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['blah abra cadabra',\n",
       " 'blah abra cadabra',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah abra ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ',\n",
       " 'blah  ']"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words='english',\n",
    "    norm='l2', # each output vector has l2 norm equal to 1\n",
    "    use_idf=True)\n",
    "\n",
    "# create a corpus with various levels of repetition of its terms\n",
    "N = 1000\n",
    "corpus = np.reshape(['blah', 'abra', 'cadabra'] * N, (N,3))\n",
    "corpus[2:,2] = ''\n",
    "corpus[int(N/2):,1] = ''\n",
    "corpus = [' '.join(corpus[i]) for i in range(N)]\n",
    "\n",
    "print('\\n'.join(corpus[:10]))\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['abra', 'blah', 'cadabra']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         abra      blah   cadabra\n",
       "0    0.238730  0.141081  0.960783\n",
       "1    0.238730  0.141081  0.960783\n",
       "2    0.860906  0.508765  0.000000\n",
       "3    0.860906  0.508765  0.000000\n",
       "4    0.860906  0.508765  0.000000\n",
       "..        ...       ...       ...\n",
       "995  0.000000  1.000000  0.000000\n",
       "996  0.000000  1.000000  0.000000\n",
       "997  0.000000  1.000000  0.000000\n",
       "998  0.000000  1.000000  0.000000\n",
       "999  0.000000  1.000000  0.000000\n",
       "\n",
       "[1000 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>abra</th>\n      <th>blah</th>\n      <th>cadabra</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.238730</td>\n      <td>0.141081</td>\n      <td>0.960783</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.238730</td>\n      <td>0.141081</td>\n      <td>0.960783</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.860906</td>\n      <td>0.508765</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.860906</td>\n      <td>0.508765</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.860906</td>\n      <td>0.508765</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>995</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>996</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>997</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>998</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>999</th>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n<p>1000 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "X_corpus_tfidf=vectorizer.fit_transform(corpus)\n",
    "\n",
    "print(vectorizer.get_feature_names()) \n",
    "\n",
    "# what happens to the weight of 'cadabra' when we vary N?\n",
    "# what happens to the weight of 'blah' and 'abra'?\n",
    "# can you explain?\n",
    "X_corpus_tfidf.toarray()[:10,:]\n",
    "\n",
    "# whereever, we always get the least weights on 'blah', this is the least relevent word to\n",
    "# look at\n",
    "# but 'cadabra' really mean something when it occurs\n",
    "pd.DataFrame(X_corpus_tfidf.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "# II. Latent Semantic Analysis\n",
    "\n",
    "## Or Truncated SVD on the TF-IDF matrix\n",
    "\n",
    "The Following code is based on \n",
    "\n",
    "- \"http://scikit-learn.org/stable/auto_examples/applications/plot_out_of_core_classification.html\": Scikit-Learn's Reuters Dataset TF-IDF + K-NN classification example\n",
    "- along with \"http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/\": Chris McCormic's LSA tutorial\n",
    "- and his \"https://github.com/chrisjmccormick/LSA_Classification\": github page.\n",
    "\n",
    "The original Reuter's 21578 dataset is part of the \"http://archive.ics.uci.edu/ml/machine-learning-databases\": UCI-ML repository and can be found \"http://archive.ics.uci.edu/ml/machine-learning-databases/reuters21578-mld/reuters21578.tar.gz\". However, in this demo we are using the already \"https://github.com/chrisjmccormick/LSA_Classification/tree/master/data\" pre-processed version in Chris McCormic's github page.\n",
    "\n",
    "\n",
    "### Let's look at some real data - the Reuters Articles Corpus"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'list'>\nNumber of train docs: 4743 Number of test docs: 4858\n\nExample train labels: [['cocoa', 'el-salvador', 'usa', 'uruguay'], ['usa'], ['usa'], ['usa', 'brazil']]\n"
     ]
    }
   ],
   "source": [
    "# fname = \"LSA_Recommender_DS7\\\\raw_text_dataset.pickle\"\n",
    "# filepath = os.getcwd() + '\\\\' + fname\n",
    "filepath = r\"C:\\Users\\xxxli\\Desktop\\ResumeProjects\\Data_Science_Applications\\LSA_Recommender_DS7\\raw_text_dataset.pickle\"\n",
    "raw_text_dataset = pickle.load(open(filepath, \"rb\"))\n",
    "corpus_train, labels_train = raw_text_dataset[0], raw_text_dataset[1] \n",
    "corpus_test, labels_test = raw_text_dataset[2], raw_text_dataset[3]\n",
    "print(type(corpus_train)) # list of string\n",
    "print('Number of train docs:', len(corpus_train), 'Number of test docs:', len(corpus_test))\n",
    "print('\\nExample train labels:', labels_train[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\nThis is how a article  616  looks like:\n\n GELCO GEL> SEES FLAT 1987 PRETAX OPERATING NET\n\nGelco Corp said that, excluding the effects of a restructuring plan, it expects pre-tax operating earnings for the year to end July 31, 1987, to be about the same as those of last year. For the year ended July 31, 1986, Gelco reported pre-tax operating earnings of 14.8 mln dlrs, or 1.08 dlrs a share. However, final results will be affected by certain charges including legal and investment advisors fees, preferred stock dividends and other costs of \n\nAnd these are its topic labels or tags:\n\n ['earn', 'usa']\n"
     ]
    }
   ],
   "source": [
    "# n = 4610\n",
    "# n = 1238\n",
    "n = np.random.choice(len(corpus_train))\n",
    "\n",
    "print('\\nThis is how a article ', n,' looks like:\\n\\n', \n",
    "      corpus_train[n][:500])\n",
    "\n",
    "print('\\nAnd these are its topic labels or tags:\\n\\n', \n",
    "      labels_train[n][:500])"
   ]
  },
  {
   "source": [
    "### TF-IDF vectorizer step:\n",
    "The TfidfVectorizer below does the following:\n",
    "- TF Step\n",
    "    - Strips out “stop words”, e.g. frequently occuring english words\n",
    "    - Filters out terms that occur in more than half of the docs\n",
    "    (max_df=0.5)\n",
    "    - Filters out terms that occur in only one document (min_df=2).\n",
    "    - Selects the 10,000 most frequently occuring words in the corpus.\n",
    "    - Normalizes the vector to account for the effect of document\n",
    "    length on the tf-idf values. Here we use l1 norm which normalized\n",
    "    by the document length\n",
    "- IDF Step\n",
    "    - Nomalize each "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(4743, 10000)\nfirst 10 features: ['a300', 'a330', 'a340', 'aa', 'aaa', 'aapl', 'ab', 'abandon', 'abandoned', 'abandonment']\nlast 10 features: ['zinc', 'zntl', 'zoete', 'zone', 'zones', 'zorinsky', 'zortman', 'zuckerman', 'zurich', 'zy']\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      a300  a330  a340   aa  aaa  aapl   ab  abandon  abandoned  abandonment  \\\n",
       "0      0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "1      0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "2      0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "3      0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "4      0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "...    ...   ...   ...  ...  ...   ...  ...      ...        ...          ...   \n",
       "4738   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "4739   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "4740   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "4741   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "4742   0.0   0.0   0.0  0.0  0.0   0.0  0.0      0.0        0.0          0.0   \n",
       "\n",
       "      ...  zinc  zntl  zoete      zone  zones  zorinsky  zortman  zuckerman  \\\n",
       "0     ...   0.0   0.0    0.0  0.050365    0.0       0.0      0.0        0.0   \n",
       "1     ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "2     ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "3     ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "4     ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "...   ...   ...   ...    ...       ...    ...       ...      ...        ...   \n",
       "4738  ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "4739  ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "4740  ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "4741  ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "4742  ...   0.0   0.0    0.0  0.000000    0.0       0.0      0.0        0.0   \n",
       "\n",
       "      zurich   zy  \n",
       "0        0.0  0.0  \n",
       "1        0.0  0.0  \n",
       "2        0.0  0.0  \n",
       "3        0.0  0.0  \n",
       "4        0.0  0.0  \n",
       "...      ...  ...  \n",
       "4738     0.0  0.0  \n",
       "4739     0.0  0.0  \n",
       "4740     0.0  0.0  \n",
       "4741     0.0  0.0  \n",
       "4742     0.0  0.0  \n",
       "\n",
       "[4743 rows x 10000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a300</th>\n      <th>a330</th>\n      <th>a340</th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aapl</th>\n      <th>ab</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>abandonment</th>\n      <th>...</th>\n      <th>zinc</th>\n      <th>zntl</th>\n      <th>zoete</th>\n      <th>zone</th>\n      <th>zones</th>\n      <th>zorinsky</th>\n      <th>zortman</th>\n      <th>zuckerman</th>\n      <th>zurich</th>\n      <th>zy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.050365</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>4738</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4739</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4740</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4741</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4742</th>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>4743 rows × 10000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "    max_features=10000,\n",
    "    min_df=2, # ignore terms which occur in less than 2 documents\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "#     token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b'\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "    )\n",
    "# play around to see what kind of token pattern actually works the best for for this corpus.\n",
    "# note how changing the token_pattern changes the output below\n",
    "\n",
    "# train a vectorizer using our corpus training set\n",
    "X_train_tfidf = vectorizer.fit_transform(corpus_train)\n",
    "# print(vectorizer.idf_)\n",
    "\n",
    "# we want to find an article in X_train_tfidf contains a word \"cocao':\n",
    "print(X_train_tfidf.shape)\n",
    "print('first 10 features:', vectorizer.get_feature_names()[:10])\n",
    "print('last 10 features:', vectorizer.get_feature_names()[-10:])\n",
    "\n",
    "pd.DataFrame(X_train_tfidf.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "10  documents found for target word:  cocoa\ndocument  266 :\nINDONESIAN TEA, COCOA EXPORTS SEEN UP, COFFEE DOWN\n\nIndonesia's exports of tea and cocoa will continue to rise in calendar 1987 but coffee exports are forecast to dip slightly in 1987/88 (April-March) as the government tries to improve quality, the U.S. Embassy said. The embassy's annual report on Indonesian agriculture forecast coffee output in 1986/87 would be 5.77 mln bags of 60 kilograms each. That is slightly less than the 5.8 mln bags produced in 1985/86. In 1987/88 coffee production is forecast to rise again to 5.8 mln bags, but exports to dip to 4.8 mln from around 5.0 mln in 1986/87. Exports in 1985/86 were 4.67 mln bags. The embassy report says coffee stocks will rise to 1.3 mln tonnes in 1987/88 from 1.15 mln in 1986/87. It bases this on a fall in exports as a result of the \"probable\" re-introduction of quotas by the International Coffee Organisation. Cocoa production and exports are forecast to rise steadily as the government develops cocoa plantations. Production of cocoa in Indonesia increased to 32,378 tonnes in calendar 1985 from 10,284 tonnes in 1980. It is projected by the government to rise to more than 50,000 tonnes by 1988. Production in 1986 is estimated by the embassy at 35,000 tonnes, as against 38,000 tonnes in 1987. The report forecasts cocoa exports to rise to 35,000 tonnes this year, from 33,000 tonnes in 1986 and 31,000 in 1985. The Netherlands is at present the biggest importer of Indonesian cocoa beans. The report forecasts that in calendar 1987, Indonesia's CTC (crushed, torn and curled) tea exports will increase significantly with the coming on stream of at least eight new CTC processing plants. Indonesia plans to diversify its tea products by producing more CTC tea, the main component of tea bags. Production of black and green teas is forecast in the embassy report to rise to 125,000 tonnes in calendar 1987 from 123,000 tonnes in 1986. Exports of these teas are likely to rise to 95,000 tonnes in 1987 from 85,000 in 1986 and around 90,000 in 1985. The embassy noted the ministry of trade tightened quality controls on tea in October 1986 in an effort to become more competititve in the world market. REUTER \n"
     ]
    }
   ],
   "source": [
    "# let's look at documents that contain a target wordthe word '\n",
    "# target_word = 'bullish'\n",
    "target_word = 'cocoa'\n",
    "\n",
    "# find our which columns of this matrix it belongs\n",
    "# then we gonna find one of these documents actually contains that word cocoa. \n",
    "doc_idx = X_train_tfidf[\n",
    "    :, vectorizer.vocabulary_.get(target_word)].nonzero()[0].tolist()\n",
    "\n",
    "print(len(doc_idx), ' documents found for target word: ', target_word)\n",
    "# And then, we can see there were there were 10 documents that were found to contain cocoa. \n",
    "i = np.random.choice(len(doc_idx))\n",
    "\n",
    "# It's one of them.\n",
    "print('document ', doc_idx[i], ':')\n",
    "print(corpus_train[doc_idx[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<4743x10000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 217725 stored elements in Compressed Sparse Row format>"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "X_train_tfidf"
   ]
  },
  {
   "source": [
    "### Truncated SVD"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Performing dimensionality reduction using LSA\n",
      "  done in 4.357sec\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPerforming dimensionality reduction using LSA\")\n",
    "t0 = time.time()\n",
    "\n",
    "# Project the tfidf vectors onto the first N principal components.\n",
    "# Though this is significantly fewer features than the original tfidf vector,\n",
    "# they are stronger features, and the accuracy is higher.\n",
    "svd = TruncatedSVD(\n",
    "    n_components=200,\n",
    "    random_state=42,\n",
    "    algorithm='arpack' \n",
    "    # if we use arpack algo, we dont have to specify random_state,\n",
    "    # because it is a determinisitc algo\n",
    "    # if you use a random projection algo\n",
    ")\n",
    "\n",
    "# making a LSA pipline by transforming X_train_tfidf, and ending with this matrix X_train_lsa\n",
    "lsa = make_pipeline(\n",
    "    svd, \n",
    "#     Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    ")\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "print(\"  done in %.3fsec\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(4743, 200)"
      ]
     },
     "metadata": {},
     "execution_count": 32
    }
   ],
   "source": [
    "X_train_lsa.shape \n",
    "#4743 articals, 200 PCs/factors we inputed. This is the matrix that contains our loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  Explained variance of the SVD step: 40%\n"
     ]
    }
   ],
   "source": [
    "# what variance we are explaining when we are looking at 200 factors\n",
    "explained_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"  Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
    "\n",
    "\n",
    "# Now apply the transformations to the test data as well.\n",
    "# note that we are using the transform method only\n",
    "X_test_tfidf = vectorizer.transform(corpus_test)\n",
    "X_test_lsa = lsa.transform(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((4858, 200), (4858, 10000))"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "X_test_lsa.shape, X_test_tfidf.shape\n",
    "# the right one is a much larger matrix. And because of that, \n",
    "# it means that we're gonna have a lot more noisy output\n",
    "# because right one is a lot sparser matrix than keft one."
   ]
  },
  {
   "source": [
    "### use this approach and SVD to create a recommender"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "-----------------------------------------------------------\n",
    "# My LSA-based Recommender:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.sparse.csr as csr\n",
    "import scipy.sparse as sparse\n",
    "from sklearn.base import clone\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "source": [
    "### 1. LSA-based Recommender.\n",
    "In this assignment, you can use and modify the LSA notebook that we covered in class.\n",
    "Start by downloading the Reuters 10K article [corpus raw_text_dataset.pickle] from https://github.com/chrisjmccormick/LSA_Classification."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Data\n",
    "filepath = filepath = r\"C:\\Users\\xxxli\\Desktop\\ResumeProjects\\Data_Science_Applications\\LSA_Recommender_DS7\\raw_text_dataset.pickle\"\n",
    "raw_text_dataset = pickle.load(open(filepath, \"rb\"))\n",
    "corpus_train, labels_train = raw_text_dataset[0], raw_text_dataset[1] \n",
    "corpus_test, labels_test = raw_text_dataset[2], raw_text_dataset[3]\n",
    "\n",
    "corpus = corpus_train + corpus_test\n",
    "corpus_train = corpus # use all the 10K articles corpus"
   ]
  },
  {
   "source": [
    "### (a) Create a ``doc2vec(doc, tfidf_vectorizer)`` function corresponding to a TFIDF vectorizerer where:\n",
    "INPUTS: ``doc``, ``tfidf_vectorizer``\n",
    "* ``doc``: any string\n",
    "* ``tfidf_vectorizer``: a TfidfVectorizer instance\n",
    "\n",
    "OUTPUTS: ``vec``, ``doc_features``, ``doc_counts``\n",
    "* ``vec``: a vector with L2 norm of 1\n",
    "* ``doc_features``: the features after tokenization and pre-processing\n",
    "* ``doc_counts``: the counts of each feature in this document\n",
    "\n",
    "Train your ``tfidf_vectorizer`` on the Reuters 10K article corpus.\n",
    "\n",
    "#### To get the counts of each feature/word, I also instantiate a ``CountVectorizer``:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "There is 9601 documents in training set.\n--------------------\nFrom the term matrix shape: (9601, 10000)\n there is 9601 documents \n with 10000 terms or features\n--------------------\nSome examples: \nfirst 10 features: ['a300', 'a320', 'a330', 'a340', 'aa', 'aaa', 'aapl', 'ab', 'abandon', 'abandoned']\nlast 10 features: ['zim', 'zimbabwe', 'zinc', 'ziyang', 'zoete', 'zone', 'zones', 'zorinsky', 'zuckerman', 'zurich']\n--------------------\nExplicitly in pandas:\n(9601, 10000)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "         a300  a320  a330  a340  aa  aaa  aapl  ab  abandon  abandoned  ...  \\\n",
       "doc idx                                                                 ...   \n",
       "0           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "1           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "2           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "3           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "4           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "5           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "6           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "7           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "8           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "9           0     0     0     0   0    0     0   0        0          0  ...   \n",
       "10          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "11          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "12          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "13          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "14          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "15          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "16          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "17          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "18          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "19          0     0     0     0   0    0     0   0        0          0  ...   \n",
       "\n",
       "         zim  zimbabwe  zinc  ziyang  zoete  zone  zones  zorinsky  zuckerman  \\\n",
       "doc idx                                                                         \n",
       "0          0         0     0       0      0     1      0         0          0   \n",
       "1          0         0     0       0      0     0      0         0          0   \n",
       "2          0         0     0       0      0     0      0         0          0   \n",
       "3          0         0     0       0      0     0      0         0          0   \n",
       "4          0         0     0       0      0     0      0         0          0   \n",
       "5          0         0     0       0      0     0      0         0          0   \n",
       "6          0         0     0       0      0     0      0         0          0   \n",
       "7          0         0     0       0      0     0      0         0          0   \n",
       "8          0         0     0       0      0     0      0         0          0   \n",
       "9          0         0     0       0      0     0      0         0          0   \n",
       "10         0         0     0       0      0     0      0         0          0   \n",
       "11         0         0     0       0      0     0      0         0          0   \n",
       "12         0         0     0       0      0     0      0         0          0   \n",
       "13         0         0     0       0      0     0      0         0          0   \n",
       "14         0         0     0       0      0     0      0         0          0   \n",
       "15         0         0     0       0      0     0      0         0          0   \n",
       "16         0         0     0       0      0     0      0         0          0   \n",
       "17         0         0     0       0      0     0      0         0          0   \n",
       "18         0         0     0       0      0     0      0         0          0   \n",
       "19         0         0     0       0      0     0      0         0          0   \n",
       "\n",
       "         zurich  \n",
       "doc idx          \n",
       "0             0  \n",
       "1             0  \n",
       "2             0  \n",
       "3             0  \n",
       "4             0  \n",
       "5             0  \n",
       "6             0  \n",
       "7             0  \n",
       "8             0  \n",
       "9             0  \n",
       "10            0  \n",
       "11            0  \n",
       "12            0  \n",
       "13            0  \n",
       "14            0  \n",
       "15            0  \n",
       "16            0  \n",
       "17            0  \n",
       "18            0  \n",
       "19            0  \n",
       "\n",
       "[20 rows x 10000 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>a300</th>\n      <th>a320</th>\n      <th>a330</th>\n      <th>a340</th>\n      <th>aa</th>\n      <th>aaa</th>\n      <th>aapl</th>\n      <th>ab</th>\n      <th>abandon</th>\n      <th>abandoned</th>\n      <th>...</th>\n      <th>zim</th>\n      <th>zimbabwe</th>\n      <th>zinc</th>\n      <th>ziyang</th>\n      <th>zoete</th>\n      <th>zone</th>\n      <th>zones</th>\n      <th>zorinsky</th>\n      <th>zuckerman</th>\n      <th>zurich</th>\n    </tr>\n    <tr>\n      <th>doc idx</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>20 rows × 10000 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "####################### to count terms\n",
    "count_vectorizer = CountVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=10000,\n",
    "    min_df=2, \n",
    "    stop_words='english',\n",
    "    analyzer='word',\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b')\n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(corpus_train)\n",
    "features_counter = count_vectorizer.get_feature_names()\n",
    "counts_counter = X_train_count.toarray()\n",
    "\n",
    "######################### TF-IDF\n",
    "my_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5, # ignore terms which occur in more than half of the documents\n",
    "    max_features=10000,\n",
    "    min_df=2, # ignore terms which occur in less than 2 documents\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b')\n",
    "\n",
    "# train a vectorizer using our corpus training set\n",
    "X_train_tfidf = my_tfidf_vectorizer.fit_transform(corpus_train)\n",
    "features_tfidf = my_tfidf_vectorizer.get_feature_names()\n",
    "\n",
    "# print results\n",
    "print('There is '+str(len(corpus_train))+' documents in training set.')\n",
    "print('-'*20)\n",
    "print('From the term matrix shape: '+str(X_train_tfidf.shape)+ '\\n there is '+str(X_train_tfidf.shape[0])+' documents \\n with '+str(X_train_tfidf.shape[1])+' terms or features')\n",
    "print('-'*20)\n",
    "print('Some examples: \\nfirst 10 features:', features_tfidf[:10])\n",
    "print('last 10 features:', features_tfidf[-10:])\n",
    "print('-'*20)\n",
    "print('Explicitly in pandas:')\n",
    "counts_df = pd.DataFrame(counts_counter, columns = features_counter)\n",
    "#counts_df = pd.DataFrame(counts_tfidf, columns = features_tfidf)\n",
    "counts_df.index.name = 'doc idx'\n",
    "print(counts_df.shape)\n",
    "counts_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "X_words = my_tfidf_vectorizer.inverse_transform(X_train_tfidf) ## this will give you words instead of tfidf where tfidf > 0\n",
    "\n",
    "tokenizer = my_tfidf_vectorizer.build_tokenizer() ## return tokenizer function used in tfidfvectorizer\n",
    "\n",
    "for idx,words in enumerate(X_words):\n",
    "    for word in words:\n",
    "        count = tokenizer(corpus_train[idx]).count(word)\n",
    "        print(idx,word,count)\n",
    "'''\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs2vec(doc, tfidf_vectorizer):\n",
    "    vec = tfidf_vectorizer.transform(doc)\n",
    "    #doc_features = tfidf_vectorizer.get_feature_names()\n",
    "    #doc_counts = vec.toarray()\n",
    "    \n",
    "    # counter\n",
    "    doc_features = features_counter\n",
    "    doc_counts = count_vectorizer.transform(doc).toarray()\n",
    "    \n",
    "    return vec, doc_features, doc_counts"
   ]
  },
  {
   "source": [
    "### (b) For each of the following doc strings, calculate their corresponding vectors\n",
    "* doc1: “Jabberwocky”\n",
    "* doc2: “buy MSFT sell AAPL hold Brent”\n",
    "* doc3: “bullish stocks”\n",
    "* doc4: “Some random forests produce deterministic losses”"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--------------------Jabberwocky--------------------\nL2 norm vector is\n\n        a300  plough  pledge  pledged  pledges  plenmeer  plenty\nCounts     0       0       0        0        0         0       0\n--------------------buy MSFT sell AAPL hold Brent--------------------\nL2 norm vector is\n  (0, 8065)\t0.29271261821048594\n  (0, 4172)\t0.358000284990265\n  (0, 1210)\t0.281467434058663\n  (0, 1091)\t0.5774997449206991\n  (0, 6)\t0.611085302775489\n        sell  brent  aapl  hold  buy  pnc  plessis\nCounts     1      1     1     1    1    0        0\n--------------------bullish stocks--------------------\nL2 norm vector is\n  (0, 8609)\t0.5821471771382473\n  (0, 1176)\t0.8130834300057836\n        bullish  stocks  a300  plessis  pleased  pledge  pledged\nCounts        1       1     0        0        0       0        0\n--------------------Some random forests produce deterministic losses--------------------\nL2 norm vector is\n  (0, 7169)\t0.5780527421462479\n  (0, 6899)\t0.3849996487707055\n  (0, 5262)\t0.3347424493598882\n  (0, 3575)\t0.6368498962394351\n        produce  random  forests  losses  a300  plenty  pledge\nCounts        1       1        1       1     0       0       0\n"
     ]
    }
   ],
   "source": [
    "test_sample = ['Jabberwocky',\n",
    "               'buy MSFT sell AAPL hold Brent',\n",
    "               'bullish stocks',\n",
    "               'Some random forests produce deterministic losses']\n",
    "\n",
    "# print results for each test doc:\n",
    "for doc in test_sample:\n",
    "    print('-'*20+doc+'-'*20)\n",
    "    v,f,c = docs2vec([doc], my_tfidf_vectorizer)\n",
    "    print('L2 norm vector is')\n",
    "    print(v)\n",
    "    \n",
    "    df2 = pd.DataFrame(c,columns=f,index=['Counts'])\n",
    "    df3 = df2.sort_values(by='Counts',axis=1,ascending=False)\n",
    "    print(df3[df3.columns[:7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the result from part b\n",
    "b_output_vec = docs2vec(test_sample, my_tfidf_vectorizer)[0]"
   ]
  },
  {
   "source": [
    "### (c) Implement a function ``recommend(vec, X_model, X_corpus, lsa_instance)`` which projects any document vector onto a given ``X_model`` where \n",
    "* ``X_model = {X_train_tfidf, X_train_lsa}``\n",
    "and returns ``doc_vec``, ``idx_top10``, ``sim_top10``, ``X_top10`` where\n",
    "* ``doc_vec``: the (sparse) vector of similarity scores of ``vec`` and members of ``X_model``. This vector should be size D × 1: 4743 number of documents in training set\n",
    "* ``idx_top10``: the indices of the top-10 similarity scores\n",
    "* ``sim_top10``: the top-10 similarity scores\n",
    "* ``X_top10``: the top-10 corpus articles most similar to the input model"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Remark: what we have now are\n",
    "* ``my_tfidf_vectorizer`` trained.\n",
    "* ``b_output_vec``: ``docs2vec(test_sample, my_tfidf_vectorizer)[0]`` or ``my_tfidf_vectorizer.transform(test_sample)``\n",
    "\n",
    "\n",
    "**our target: Find top 10 docs in corpus_train (9601 docs) that are most similar to our ``test_sample`` of 4 strings.**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "##### TF-IDF:\n",
    "* Model_X: ``TrainVectorizationArray_tfidf``\n",
    "* Test_vec: ``TestVectorizationArray_tfidf``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "TrainVectorizationArray_tfidf = X_train_tfidf.toarray()\n",
    "TestVectorizationArray_tfidf = b_output_vec.toarray()"
   ]
  },
  {
   "source": [
    "##### LSA:\n",
    "\n",
    "In the cell below, we perform **dimensionality reduction using LSA**:\n",
    "* Project the tfidf vectors ``X_train_tfidf`` onto the first $N=200$ principal components.\n",
    "* Though this is significantly fewer features (200) than the original tfidf vector (10000), they are stronger features, and the accuracy is higher.\n",
    "\n",
    "\n",
    "* Model_X: ``TrainVectorizationArray_lsa``\n",
    "* Test_vec: ``TestVectorizationArray_lsa``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate svd instance\n",
    "svd = TruncatedSVD(\n",
    "    n_components=200,\n",
    "    random_state=42,\n",
    "    algorithm='arpack' \n",
    ")\n",
    "\n",
    "# making a LSA pipline by transforming X_train_tfidf, and ending with this matrix X_train_lsa\n",
    "lsa = make_pipeline(\n",
    "    svd, \n",
    "#   Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    ")\n",
    "\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "TrainVectorizationArray_lsa = X_train_lsa\n",
    "TestVectorizationArray_lsa = lsa.transform(b_output_vec) # lsa.trandorm(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(9601, 10000)\n(9601, 200)\n(4, 10000)\n(4, 200)\n(4, 10000)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(TrainVectorizationArray_tfidf))\n",
    "print(np.shape(TrainVectorizationArray_lsa))\n",
    "print(np.shape(TestVectorizationArray_tfidf))\n",
    "print(np.shape(TestVectorizationArray_lsa))\n",
    "print(np.shape(b_output_vec))"
   ]
  },
  {
   "source": [
    "##### Put together and construct the ``recommend()`` method:\n",
    "* Assume we use **Cosine Similarity score on normalized vectors**\n",
    "$$ score(A,B) = \\frac{A\\cdot B}{||A||\\cdot ||B||} = A\\cdot B$$\n",
    "* To calculate/project any doc vector ``vec`` onto a given ``X_model``:\n",
    "\n",
    "``score = vec.dot(X_model)``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = [X_train_tfidf, X_train_lsa]"
   ]
  },
  {
   "source": [
    "#### Recommend:\n",
    "What we already have:\n",
    "* trained tfidf: ``my_tfidf_vectorizer``\n",
    "* trained lsa: ``lsa``, input is ``lsa_instant``\n",
    "* ``vec`` :`` b_output_vec = my_tfidf_vectorizer.transform(test_sample)``\n",
    "* fitted and transformed trainning sets: ``X_model = [X_train_tfidf, X_train_lsa]``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(vec, X_model, X_corpus, lsa_instance):\n",
    "    '''\n",
    "    Args\n",
    "    ----------\n",
    "    vec = b_output_vec = my_tfidf_vectorizer.transform(test_sample)\n",
    "    X_model = [X_train_tfidf, X_train_lsa]\n",
    "    X_corpus = corpus_train\n",
    "    lsa_instance: lsa trained above\n",
    "    \n",
    "    '''\n",
    "    TrainVectorizationArray_tfidf = X_model[0].toarray()\n",
    "    TrainVectorizationArray_lsa = X_model[1]\n",
    "    \n",
    "    TestVectorizationArray_tfidf = vec.toarray()\n",
    "    TestVectorizationArray_lsa = lsa_instance.transform(vec)\n",
    "    \n",
    "    # scores\n",
    "    score_tfidf = TestVectorizationArray_tfidf.dot(TrainVectorizationArray_tfidf.T)\n",
    "    score_tfidf_vec = score_tfidf.sum(0)\n",
    "    score_lsa = TestVectorizationArray_lsa.dot(TrainVectorizationArray_lsa.T)\n",
    "    score_lsa_vec = score_lsa.sum(0)\n",
    "    \n",
    "    df1 = pd.DataFrame(score_tfidf_vec, columns=['score tfidf'])\n",
    "    df2 = pd.DataFrame(score_lsa_vec, columns=['score lsa'])\n",
    "    \n",
    "    ########## tfidf\n",
    "    # top10 index\n",
    "    res1 = df1.sort_values(by='score tfidf',ascending=False).head(10)\n",
    "    idx1 = res1.index.values\n",
    "    \n",
    "    # top10 values\n",
    "    s1 = res1['score tfidf'].values\n",
    "    \n",
    "    # top10 articles\n",
    "    art1 = []\n",
    "    for i in idx1:\n",
    "        #print('\\n -------This is how a article index ', i,' looks like:-------\\n\\n', corpus_train[i][:100])\n",
    "        art1.append(X_corpus[i][:200])\n",
    "    \n",
    "    ########## LSA\n",
    "    res2 = df2.sort_values(by='score lsa',ascending=False).head(10)\n",
    "    idx2 = res2.index.values\n",
    "    s2 = res2['score lsa'].values\n",
    "    art2 = []\n",
    "    for i in idx2:\n",
    "        #print('\\n -------This is how a article index ', i,' looks like:-------\\n\\n', corpus_train[i][:100])\n",
    "        art2.append(X_corpus[i][:200])\n",
    "    \n",
    "    '''\n",
    "    Returns\n",
    "    ---------\n",
    "    doc_vec: the (sparse) vector of similarity scores of vec and members of X_model. This vector should be size D × 1: 4743 number of documents in training set\n",
    "    idx_top10: the indices of the top-10 similarity scores\n",
    "    sim_top10: the top-10 similarity scores\n",
    "    X_top10: the top-10 corpus articles most similar to the input model\n",
    "    \n",
    "    '''\n",
    "    doc_vec = (df1, df2)\n",
    "    idx_top10 = (idx1, idx2)\n",
    "    sim_top10 = (s1, s2)\n",
    "    X_top10 = (art1, art2)\n",
    "    return doc_vec, idx_top10, sim_top10, X_top10"
   ]
  },
  {
   "source": [
    "#### What does your ``recommend()`` function output for the ``doc`` vectors in (b)? "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['Jabberwocky', 'buy MSFT sell AAPL hold Brent', 'bullish stocks', 'Some random forests produce deterministic losses']\n"
     ]
    }
   ],
   "source": [
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- similarity scores tfidf ----------\n      score tfidf\n0        0.000000\n1        0.000000\n2        0.000000\n3        0.052666\n4        0.000000\n...           ...\n9596     0.000000\n9597     0.000000\n9598     0.000000\n9599     0.000000\n9600     0.000000\n\n[9601 rows x 1 columns]\n---------- similarity scores lsa ----------\n      score lsa\n0      0.007706\n1      0.017369\n2     -0.005140\n3      0.044632\n4      0.009103\n...         ...\n9596  -0.003198\n9597  -0.002799\n9598   0.001968\n9599  -0.005570\n9600   0.012080\n\n[9601 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "doc_vec, idx_top10, sim_top10, X_top10 =  recommend(b_output_vec, X_model, corpus_train, lsa)\n",
    "print('---------- similarity scores tfidf ----------')\n",
    "print(doc_vec[0])\n",
    "print('---------- similarity scores lsa ----------')\n",
    "print(doc_vec[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- top 10 scores indices tfidf ----------\n[1542 4417 4467 1687 3751 7680 6782 8509 9102 7244]\n---------- top 10 scores indices lsa ----------\n[1682 3751 1687 3747 9219 1190 8659 6013 1202 6020]\n"
     ]
    }
   ],
   "source": [
    "print('---------- top 10 scores indices tfidf ----------')\n",
    "print(idx_top10[0])\n",
    "print('---------- top 10 scores indices lsa ----------')\n",
    "print(idx_top10[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- top 10 similarity scores tfidf ----------\n[0.34321789 0.31331889 0.31331889 0.25290315 0.24933593 0.24118304\n 0.21381491 0.2108215  0.20563376 0.20121979]\n---------- top 10 similarity scores lsa ----------\n[0.14191987 0.14012116 0.13971319 0.1388139  0.138448   0.13833028\n 0.13302639 0.13302639 0.12306137 0.12255294]\n"
     ]
    }
   ],
   "source": [
    "print('---------- top 10 similarity scores tfidf ----------')\n",
    "print(sim_top10[0])\n",
    "print('---------- top 10 similarity scores lsa ----------')\n",
    "print(sim_top10[1])"
   ]
  },
  {
   "source": [
    "#### Compare the articles recommended by each model:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "---------- top 10 articals tfidf ----------\n\n*****\nANALYST REITERATES BUY ON SOME DRUG STOCKS\n\nMerrill Lynch and Co analyst Richard Vietor said he reiterated a buy recommendation on several drug stocks today. The stocks were Bristol-Myers Co BMY>, whi\n\n*****\nSUBROTO SEES OIL MARKET CONTINUING BULLISH\n\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a pres\n\n*****\nSUBROTO SEES OIL MARKET CONTINUING BULLISH\n\nIndonesian Energy Minister Subroto said he sees the oil market continuing bullish, with underlying demand expected to rise later in the year. He told a pres\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 3.4 mln barrels in the week ended Feb 27 to 128.4 mln barrels, the Energy Information Administration\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 8.8 mln barrels in the week ended March six to 119.6 mln barrels, the Energy Information Administrat\n\n*****\nWALL STREET SURVIVES TRIPLE EXPIRATIONS\n\nThe four-times-a-year \"triple witching hour\" did not jolt Wall Street as much as it has in the past. Market averages finished sharply higher as stock index fut\n\n*****\nPARIS TO ADD THREE STOCKS TO CONTINUOUS QUOTATION\n\nThe Paris Bourse will add a further three stocks to its computerised continuous trading system from March 24, bringing the total number of continuous\n\n*****\nDRAWDOWN SEEN IN U.S. DISTILLATE STOCKS\n\nTonight's American Petroleum Institute oil inventory report is expected to show another drawdown in distillate stocks of between two and 7.5 mln barrels for th\n\n*****\nTALKING POINT/TOBACCO STOCKS\n\nStocks of tobacco companies rose sharply as investors grew more confident that an excise tax would not be imposed on tobacco, traders and analysts said. They also said th\n\n*****\nTROPICAL FOREST DEATH COULD SPARK NEW DEBT CRISIS\n\nThe death of the world's tropical rain forests could trigger a new debt crisis and social and biological disasters, scientists and ecologists involve\n\n\n\n---------- top 10 articals lsa ----------\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 3.4 MLN BBLS, GASOLINE OFF 100,000, CRUDE UP 3.2 MLN\n\n\n\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 8.8 mln barrels in the week ended March six to 119.6 mln barrels, the Energy Information Administrat\n\n*****\nEIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 3.4 mln barrels in the week ended Feb 27 to 128.4 mln barrels, the Energy Information Administration\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 8.8 MLN, GASOLINE OFF 500,000, CRUDE OFF 1.2 MLN\n\n\n\n\n*****\nEIA SAYS DISTILLATE STOCKS OFF 3.2 MLN BBLS, GASOLINE UP 2.2 MLN, CRUDE UP 7.5 MLN\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 4.4 MLN BBLS, GASOLINE OFF 30,000, CRUDE UP 700,000\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 4.07 MLN BBLS, GASOLINE OFF 2.69 MLN, CRUDE UP 8.53 MLN\n\n\n\n\n*****\nAPI SAYS DISTILLATE STOCKS OFF 7.35 MLN BBLS, GASOLINE OFF 2.89 MLN, CRUDE OFF 4.39 MLN \n\n\n\n\n*****\nAPI SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 4.4 mln barrels in the week ended Feb 27 to 127.10 mln barrels from 131.50 mln the previous week, th\n\n*****\nAPI SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\n\nDistillate fuel stocks held in primary storage fell by 7.35 mln barrels in the week ended March 13 to 112.74 mln barrels from a revised 120.09 mln the prev\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                       0  \\\n",
       "tfidf  ANALYST REITERATES BUY ON SOME DRUG STOCKS\\n\\n...   \n",
       "lsa    EIA SAYS DISTILLATE STOCKS OFF 3.4 MLN BBLS, G...   \n",
       "\n",
       "                                                       1  \\\n",
       "tfidf  SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\n...   \n",
       "lsa    EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...   \n",
       "\n",
       "                                                       2  \\\n",
       "tfidf  SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\n...   \n",
       "lsa    EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...   \n",
       "\n",
       "                                                       3  \\\n",
       "tfidf  EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...   \n",
       "lsa    EIA SAYS DISTILLATE STOCKS OFF 8.8 MLN, GASOLI...   \n",
       "\n",
       "                                                       4  \\\n",
       "tfidf  EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...   \n",
       "lsa    EIA SAYS DISTILLATE STOCKS OFF 3.2 MLN BBLS, G...   \n",
       "\n",
       "                                                       5  \\\n",
       "tfidf  WALL STREET SURVIVES TRIPLE EXPIRATIONS\\n\\nThe...   \n",
       "lsa    API SAYS DISTILLATE STOCKS OFF 4.4 MLN BBLS, G...   \n",
       "\n",
       "                                                       6  \\\n",
       "tfidf  PARIS TO ADD THREE STOCKS TO CONTINUOUS QUOTAT...   \n",
       "lsa    API SAYS DISTILLATE STOCKS OFF 4.07 MLN BBLS, ...   \n",
       "\n",
       "                                                       7  \\\n",
       "tfidf  DRAWDOWN SEEN IN U.S. DISTILLATE STOCKS\\n\\nTon...   \n",
       "lsa    API SAYS DISTILLATE STOCKS OFF 7.35 MLN BBLS, ...   \n",
       "\n",
       "                                                       8  \\\n",
       "tfidf  TALKING POINT/TOBACCO STOCKS\\n\\nStocks of toba...   \n",
       "lsa    API SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...   \n",
       "\n",
       "                                                       9  \n",
       "tfidf  TROPICAL FOREST DEATH COULD SPARK NEW DEBT CRI...  \n",
       "lsa    API SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>0</th>\n      <th>1</th>\n      <th>2</th>\n      <th>3</th>\n      <th>4</th>\n      <th>5</th>\n      <th>6</th>\n      <th>7</th>\n      <th>8</th>\n      <th>9</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>tfidf</th>\n      <td>ANALYST REITERATES BUY ON SOME DRUG STOCKS\\n\\n...</td>\n      <td>SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\n...</td>\n      <td>SUBROTO SEES OIL MARKET CONTINUING BULLISH\\n\\n...</td>\n      <td>EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n      <td>EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n      <td>WALL STREET SURVIVES TRIPLE EXPIRATIONS\\n\\nThe...</td>\n      <td>PARIS TO ADD THREE STOCKS TO CONTINUOUS QUOTAT...</td>\n      <td>DRAWDOWN SEEN IN U.S. DISTILLATE STOCKS\\n\\nTon...</td>\n      <td>TALKING POINT/TOBACCO STOCKS\\n\\nStocks of toba...</td>\n      <td>TROPICAL FOREST DEATH COULD SPARK NEW DEBT CRI...</td>\n    </tr>\n    <tr>\n      <th>lsa</th>\n      <td>EIA SAYS DISTILLATE STOCKS OFF 3.4 MLN BBLS, G...</td>\n      <td>EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n      <td>EIA SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n      <td>EIA SAYS DISTILLATE STOCKS OFF 8.8 MLN, GASOLI...</td>\n      <td>EIA SAYS DISTILLATE STOCKS OFF 3.2 MLN BBLS, G...</td>\n      <td>API SAYS DISTILLATE STOCKS OFF 4.4 MLN BBLS, G...</td>\n      <td>API SAYS DISTILLATE STOCKS OFF 4.07 MLN BBLS, ...</td>\n      <td>API SAYS DISTILLATE STOCKS OFF 7.35 MLN BBLS, ...</td>\n      <td>API SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n      <td>API SAYS DISTILLATE, GAS STOCKS OFF IN WEEK\\n\\...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "print('---------- top 10 articals tfidf ----------')\n",
    "for t in X_top10[0]:\n",
    "    print('\\n'+'*'*5)\n",
    "    print(t)\n",
    "print('\\n\\n')\n",
    "print('---------- top 10 articals lsa ----------')\n",
    "for t in X_top10[1]:\n",
    "    print('\\n'+'*'*5)\n",
    "    print(t)    \n",
    "pd.DataFrame(X_top10, index=['tfidf','lsa'])"
   ]
  },
  {
   "source": [
    "##### Compare time cost: \n",
    "* Time the process from instantiation of models to the finalization of all the results.\n",
    "* Copy paste code above since we want to do seperate work here for each of the models."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "TF-IDF is done in 2.497sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "######################### TF-IDF\n",
    "my_tfidf_vectorizer = TfidfVectorizer(max_df=0.5,max_features=10000,min_df=2,stop_words='english',norm='l2',\n",
    "    use_idf=True, analyzer='word',token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b')\n",
    "X_train_tfidf = my_tfidf_vectorizer.fit_transform(corpus_train)\n",
    "features_tfidf = my_tfidf_vectorizer.get_feature_names()\n",
    "b_output_vec = docs2vec(test_sample, my_tfidf_vectorizer)[0]\n",
    "TrainVectorizationArray_tfidf = X_train_tfidf.toarray()\n",
    "TestVectorizationArray_tfidf = b_output_vec.toarray()\n",
    "score_tfidf = TestVectorizationArray_tfidf.dot(TrainVectorizationArray_tfidf.T)\n",
    "score_tfidf_vec = score_tfidf.sum(0)\n",
    "df1 = pd.DataFrame(score_tfidf_vec, columns=['score tfidf'])\n",
    "res1 = df1.sort_values(by='score tfidf',ascending=False).head(10)\n",
    "idx1 = res1.index.values\n",
    "s1 = res1['score tfidf'].values\n",
    "art1 = []\n",
    "for i in idx1:\n",
    "    #print('\\n -------This is how a article index ', i,' looks like:-------\\n\\n', corpus_train[i][:100])\n",
    "    art1.append(corpus_train[i][:100])\n",
    "\n",
    "print(\"TF-IDF is done in %.3fsec\" % (time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Truncated SVD/LSA is done in 7.914sec\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "######################### LSA/SVD\n",
    "svd = TruncatedSVD(n_components=200,random_state=42,algorithm='arpack' )\n",
    "lsa = make_pipeline(svd)\n",
    "#   Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    "\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "TrainVectorizationArray_lsa = X_train_lsa\n",
    "TestVectorizationArray_lsa = lsa.transform(b_output_vec) # lsa.trandorm(vec)\n",
    "score_lsa = TestVectorizationArray_lsa.dot(TrainVectorizationArray_lsa.T)\n",
    "score_lsa_vec = score_lsa.sum(0)\n",
    "df2 = pd.DataFrame(score_lsa_vec, columns=['score lsa'])\n",
    "res2 = df2.sort_values(by='score lsa',ascending=False).head(10)\n",
    "idx2 = res2.index.values\n",
    "s2 = res2['score lsa'].values\n",
    "art2 = []\n",
    "for i in idx2:\n",
    "    #print('\\n -------This is how a article index ', i,' looks like:-------\\n\\n', corpus_train[i][:100])\n",
    "    art2.append(corpus_train[i][:100])\n",
    "print(\"Truncated SVD/LSA is done in %.3fsec\" % (time.time() - t0))"
   ]
  },
  {
   "source": [
    "### Conclusion:\n",
    "* Compared to TF-IDF, LSA improves the recommendation giving more relevent articles,\n",
    "* but it has no improvement in terms of time."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### (d) Extra credit: Repeat the same exercise but instead of the Reuters 10K dataset, use the following corpus of 200K English plaintext jokes: https://github.com/taivop/jokedataset. Does your recommender system actually find similar jokes? Give examples of good and bad recommendations. Provide a list of suggestions of how one could improve upon this recommender."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "#### Import data from GitHub:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "we have totally  208345 jokes\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import urllib\n",
    "\n",
    "url1=\"https://raw.githubusercontent.com/taivop/joke-dataset/master/reddit_jokes.json\"\n",
    "url2='https://raw.githubusercontent.com/taivop/joke-dataset/master/stupidstuff.json'\n",
    "url3='https://raw.githubusercontent.com/taivop/joke-dataset/master/wocka.json'\n",
    "\n",
    "urls=[url1,url2,url3]\n",
    "jokes = []\n",
    "for idx, url in enumerate(urls):\n",
    "    r=requests.get(url)\n",
    "    t=json.loads(r.content)\n",
    "    for i in range(len(t)):\n",
    "        if idx == 0:\n",
    "            jokes.append(t[i]['title']+'\\n\\n'+t[i]['body'])\n",
    "        elif idx ==1:\n",
    "            jokes.append(t[i]['category']+'\\n\\n'+t[i]['body'])\n",
    "        else:\n",
    "            jokes.append(t[i]['title']+'\\n\\n'+t[i]['body'] +'\\n\\n'+t[i]['category'])\n",
    "print('we have totally ',len(jokes), 'jokes')    "
   ]
  },
  {
   "source": [
    "##### Let's split the dataset into\n",
    "* ``joke_universe``: ``jokes[:-1]``\n",
    "* ``joke_target``: ``jokes[-1:]``"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Here, we have a universe of  208344  jokes in total.\n And we want to find 10 jokes from the universe that are the most similar to our  1  target jokes.\n\n***************\n... And We Wonder Why Everyone Hates Us\n\nCustomer: \"Are you Hispanic?\"\n\nMe: \"No.\"\n\nCustomer: \"Middle Eastern?\"\n\nMe: \"No.\"\n\nCustomer: \"Egyptian?\"\n\nMe: \"No.\"\n\nCustomer: \"What are you?\"\n\nMe: \"Chinese.\"\n\n(customer puts on offended face)\n\nCustomer: \"I don't appreciate you treating me like I'm dumb.\"\n\nMe: \"Excuse me? I'm being honest.\"\n\nCustomer: \"NO CHINESE PERSON WOULD EVER HAVE EYES AS BIG AS YOURS!!!\"\n\nMe: *mouth wide open*\n\nInsults\n"
     ]
    }
   ],
   "source": [
    "joke_universe = jokes[:-1]\n",
    "joke_target = jokes[-1:]\n",
    "print('Here, we have a universe of ',len(joke_universe),' jokes in total.\\n And we want to find 10 jokes from the universe that are the most similar to our ',len(joke_target),' target jokes.')\n",
    "for j in joke_target:\n",
    "    print('\\n'+'*'*15)\n",
    "    print(j)"
   ]
  },
  {
   "source": [
    "#### We want to find 10 jokes that are most similar with our target set.\n",
    "\n",
    "#### TF-IDF:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n -------This is how a joke index  208332  looks like:-------\n\n Paging Leonidas To The Front Desk\n\nCustomer: \"Look! My friend told me I could get this type of hammer at your store! Now go get it for me!\"\n\nCashier: \"Sir, I already told you... we don't have ANY hammers back here that aren't already stocked on the shelves.\"\n\nCustomer: \"LOOK HERE. F**K YOU! I KNOW YOU'RE TRYING TO SAVE MONEY BY SWITCHING OUT YOUR STOCKS! GET ME THIS HAMMER!\"\n\n(At this point, I come to the front of the store, overhearing what's going on; note that I'm the manager.)\n\nMe: \"Is there a problem?\"\n\nCustomer: \"Yes sir! Your employee here is not doing what I tell her to!\"\n\nMe: \"Well, you need to calm down and understand that we don't have what you're looking for. So maybe you should go back to shelves and checkâ\"\n\nCustomer: \"F**K THAT!!! IT'S NOT THERE, OKAY?! YOU NEED TO F**KING GET ME WHAT I ASK FOR!\"\n\nMe: \"That's it. Get out of my store.\"\n\nCustomer: \"What? NO!\"\n\nMe: \"Sir, get out, or I have to take you out.\"\n\nCustomer: \"Then do it!\"\n\n(I go around the counter and approach the customer. I yank him by his collar & drag him to the door.)\n\nMe: \"Now, then... you wanna apologize and maybe come back in?\"\n\nCustomer: \"No! I just want my hammer! God, what is this madness?!\"\n\nMe: *puts the customer down*\n\nCustomer: *confused* \"... What is it?\"\n\n(I turn back to the cashier, who nods in approval. I then turn back to face the customer.)\n\nMe: \"Madness? THIS! IS! SPARTAAAAAAAAA!\" *kicks customer out of store and slams door*\n\nAt Work\n\n -------This is how a joke index  84187  looks like:-------\n\n Barista asks a customer if they would like their coffee black\n\nCustomer replies \"what other colors do you have?\"\n\n -------This is how a joke index  208338  looks like:-------\n\n As The Checkout Line Churns\n\n(I'm ringing up a customer and notice her last name is the same as mine. I have a very uncommon last name, so I made the mistake of mentioning this...)\n\nMe: \"Your last name is [name]? Mine, too. Wonder if we're related?\" *chuckle*\n\nCustomer: *very serious* \"What is your name?\"\n\nMe: \"Oh, I was joking, we're not related; almost all of my family lives up in New England.\"\n\nCustomer: *more serious* \"What is your name?\"\n\nMe: \"Uhhh...I'm noâ\"\n\nCustomer: \"Do you have a brother named [brother's name]?\"\n\nMe: \"Yes, actually...\"\n\nCustomer: \"Is your mother [mom's name]?\"\n\nMe: \"Uh, yeah...\"\n\nCustomer: \"And your father's name is [my estranged father's name]?\"\n\nMe: \"Well, he's my biological father, yes.\"\n\nCustomer: *sticks out hand* \"Nice to meet you, I'm your step-mother!\"\n\n(The entire line of about a dozen people behind her gasps, like they were watching a soap opera.)\n\nMe: \"Oh, God...please don't tell my father I work here.\"\n\nCustomer: \"You know why your father left your mother, right?\"\n\nMe: \"Uh...no?\"\n\nCustomer: \"Because she cheated on him with [my stepfather]!\"\n\n(The line behind her gasps again.)\n\nMe: \"Oh, okay...\"\n\nCustomer: \"You know, your father is very heartbroken about you. You've grown up to be such a beautiful young woman. You should call him and talk to him just so he can see how you're doing.\"\n\nMe: \"Actually, we don'tâ\"\n\nCustomer: \"You and I need to go out for coffee sometime. I have a lot of stories to tell you.\"\n\nMe: \"Okay, wellâ\"\n\nCustomer: \"I promise, I'm not an evil stepmother. Well, I'll see you later, sweetie!\" *bounces out the front door*\n\nMe: *speechless*\n\nNext customer: \"Sweetie, are you okay?\"\n\nMe: *still speechless*\n\nNext customer: \"Why don't you take a break? We don't mind waiting.\"\n\nEntire line: \"No! Go take a break!\"\n\nMe, to my boss: \"Hey, I'm taking a break. I'll be back inâ\"\n\nBoss: \"For God's sake, go home! I'll see you on Monday.\"\n\nMen / Women\n\n -------This is how a joke index  51040  looks like:-------\n\n Customer Service\n\nA customer approaches the customer service desk and looks at the representative's name-badge.\n\nThe customer points to the badge and says, \"How do you say?\"\n\nThe representative tilts his head and says, \"...Pat.\"\n\nThe customer nods and says, \"Pat. You say name for me?\"  \nThe customer pulls a paper out of his pocket.\n\nPat looks at the name and exclaims, \"Jesus!\"\n\n\"Jesus, thanks Pat.\"\n\n\n -------This is how a joke index  143623  looks like:-------\n\n Store Policy\n\nCUSTOMER: I'd like to buy some dog food.\nCHECKOUT: Do you have a dog?\nCUSTOMER: Yes.\nCHECKOUT: Where is he?\nCUSTOMER: He's at home.\nCHECKOUT LADY: I'm sorry, I can't sell this dog food to you unless I see the dog. Store policy.\n\nThe next day, the customer returns.\nCUSTOMER: I'd like to buy some cat food.\nCHECKOUT: Do you have a cat?\nCUSTOMER: Yes.\nCHECKOUT: Well...where is he?\nCUSTOMER: He's at home!\nCHECKOUT: Sorry, I can't sell this cat food to you unless I see your cat.\n\nThe next day the customer returns.\nCHECKOUT: What's in the sack?\nCUSTOMER: Put your hand inside.\nCHECKOUT: Hmmm...It's warm and moist! What is it?\nCUSTOMER: I would like to buy some toilet paper.\n\n -------This is how a joke index  173384  looks like:-------\n\n A customer asked me for a good reliable printer...\n\n\n\n -------This is how a joke index  168439  looks like:-------\n\n Comcast's Customer Service\n\nba dum tsss\n\n -------This is how a joke index  92982  looks like:-------\n\n Valve improving their customer service.\n\n\n\n -------This is how a joke index  143184  looks like:-------\n\n What did the server say when the customer requested something they didn't have?\n\n404\n\n -------This is how a joke index  198839  looks like:-------\n\n Cars\n\nWhat if people bought cars like they buy\nComputers?\n\nThe car companies don't have help lines\nfor people who don't know how to drive,\nbecause people don't buy cars like they\nbuy computers, imagine if they did.....\n\nHelpline: General Motors Helpline, how can I help\nyou?\n\nCustomer: I got in my car and closed the door and\nnothing happened!\n\nHelpline: Did you put the key in the ignition slot\nand turn it?\n\nCustomer: What's an ignition?\n\nHelpline: It's a starter motor that draws current\nfrom your battery and turns over the engine.\n\nCustomer: Ignition? Motor? Battery? Engine? How\ncome I have to know all these technical terms to\nuse my car.\n\nHelpline: Toyota Helpline, how can I help you?\n\nCustomer: My car ran fine for a week and now it\nwon't go anywhere!\n\nHelpline: Is the gas tank empty?\n\nCustomer: Huh?  How do I know?\n\nHelpline: There's a little gauge on the front\npanel with a needle and markings of 'E' and 'F'.\nWhere is the needle pointing?\n\nCustomer: It's pointing to 'E'. What does that\nmean?\n\nHelpline: It means you have to visit a gasoline\nvendor and purchase some more gasoline. You can\ninstall it yourself or pay the vendor to install\nit for you.\n\nCustomer: What? I paid $18,000 for this car!\nAnd you're telling me I need to keep buying more\ncomponents? This is outrageous! I want a car that\ncomes with everything built in!\n\nHelpline: Ford Helpline, how can I help you?\n\nCustomer: Your cars suck!\n\nHelpline: What's wrong?\n\nCustomer: It crashed, that's what's wrong!\n\nHelpline: What were you doing?\n\nCustomer: Well I wanted to go faster, so I pushed\nthe accelerator pedal all the way to the floor, it\nworked for a while and then it when off the road\nat a corner and crashed and it won't start now!\n\nHelpline: It's your responsibility if you misuse\nthe product. What do you expect us to do about it?\n\nCustomer: I expect you to send me one of the\nlatest versions that doesn't crash!\n\nHelpline: BMW Helpline, how can I help you?\n\nCustomer: Hi, I just bought my first car, and I\nchose your car because it has automatic\ntransmission, cruise control, power steering,\npower brakes, power door locks, power seats,\npower..\n\nHelpline:  Well,.. thanks for buying one of our\ntop of line cars. So how can I help you?\n\nCustomer: Well, how do I work it?\n\nHelpline: Do you know how to drive?\n\nCustomer: Do I know how to what?\n\nHelpline: Do you know how to drive?\n\nCustomer: Look, I'm not a mechanic. I'm not even\nvery technical.  I just want to go places in my\nnew car!\n\nOther / Misc\n"
     ]
    }
   ],
   "source": [
    "joke_tfidf_vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    max_features=3000,\n",
    "    min_df=2,\n",
    "    stop_words='english',\n",
    "    norm='l2',\n",
    "    use_idf=True, \n",
    "    analyzer='word',\n",
    "    token_pattern = '(?u)\\\\b[a-zA-Z]\\\\w+\\\\b'\n",
    "    )\n",
    "\n",
    "joke_X_train_tfidf = joke_tfidf_vectorizer.fit_transform(joke_universe)\n",
    "joke_features_tfidf = joke_tfidf_vectorizer.get_feature_names()\n",
    "joke_b_output_vec = joke_tfidf_vectorizer.transform(joke_target)\n",
    "joke_TrainVectorizationArray_tfidf = joke_X_train_tfidf.toarray()\n",
    "joke_TestVectorizationArray_tfidf = joke_b_output_vec.toarray()\n",
    "\n",
    "joke_score_tfidf = joke_TestVectorizationArray_tfidf.dot(joke_TrainVectorizationArray_tfidf.T)\n",
    "joke_score_tfidf_vec = joke_score_tfidf.sum(0)\n",
    "\n",
    "joke_df1 = pd.DataFrame(joke_score_tfidf_vec, columns=['score tfidf'])\n",
    "\n",
    "joke_res1 = joke_df1.sort_values(by='score tfidf',ascending=False).head(10)\n",
    "joke_idx1 = joke_res1.index.values\n",
    "joke_s1 = joke_res1['score tfidf'].values\n",
    "joke_art1 = []\n",
    "for i in joke_idx1:\n",
    "    print('\\n -------This is how a joke index ', i,' looks like:-------\\n\\n', joke_universe[i])\n",
    "    joke_art1.append(joke_universe[i])"
   ]
  },
  {
   "source": [
    "#### Truncated SVD/LSA:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n -------This is how a joke index  39623  looks like:-------\n\n what do you call a Chinese person with down syndrome?\n\nSom ting wong\n\n -------This is how a joke index  22833  looks like:-------\n\n How do you blindfold a chinese person?\n\nWith dental floss\n\n -------This is how a joke index  22040  looks like:-------\n\n What do you call a Chinese Millionaire?\n\nCha Ching\n\n -------This is how a joke index  137715  looks like:-------\n\n What do you call a foreigner who is obsessed with Chinese culture?\n\nA zhuologist\n\n -------This is how a joke index  119795  looks like:-------\n\n What do you call a Chinese Podiatrist?\n\nHee Lan To\n\n -------This is how a joke index  121680  looks like:-------\n\n What part of your punctuality emancipates the Chinese?\n\nYour Ti\"ming\"!\n\n -------This is how a joke index  25922  looks like:-------\n\n What do you call a Chinese millionaire?\n\nCha Ching\n\n -------This is how a joke index  117910  looks like:-------\n\n What do Chinese lumberjacks do?\n\nChopsticks\n\n -------This is how a joke index  45799  looks like:-------\n\n what do you call a chinese millionaire?\n\nCha-Ching\n\n -------This is how a joke index  141260  looks like:-------\n\n What do you call a chinese millionaire?\n\nCha-Ching!\n"
     ]
    }
   ],
   "source": [
    "joke_svd = TruncatedSVD(n_components=200,random_state=42,algorithm='arpack')\n",
    "joke_lsa = make_pipeline(joke_svd) \n",
    "#   Normalizer(copy=False) # try commenting this out. Do you get a better result?\n",
    "\n",
    "joke_X_train_lsa = joke_lsa.fit_transform(joke_X_train_tfidf)\n",
    "joke_TrainVectorizationArray_lsa = joke_X_train_lsa\n",
    "joke_TestVectorizationArray_lsa = joke_lsa.transform(joke_b_output_vec)\n",
    "joke_score_lsa = joke_TestVectorizationArray_lsa.dot(joke_TrainVectorizationArray_lsa.T)\n",
    "joke_score_lsa_vec = joke_score_lsa.sum(0)\n",
    "\n",
    "joke_df2 = pd.DataFrame(joke_score_lsa_vec, columns=['score lsa'])\n",
    "\n",
    "joke_res2 = joke_df2.sort_values(by='score lsa',ascending=False).head(10)\n",
    "joke_idx2 = joke_res2.index.values\n",
    "joke_s2 = joke_res2['score lsa'].values\n",
    "joke_art2 = []\n",
    "for i in joke_idx2:\n",
    "    print('\\n -------This is how a joke index ', i,' looks like:-------\\n\\n', joke_universe[i])\n",
    "    joke_art2.append(joke_universe[i])"
   ]
  },
  {
   "source": [
    "### Conclusion:\n",
    "* Accidently, my joke target is kind of about sensitive topic under ``insult`` category I guess. Sorry about that.\n",
    "* But if we assume that I want to find out all of those most impolite (impolite to chinese people) jokes, and want to delete them immediately, which models give me better delete recommendations?\n",
    "* According to what I printed above, obviously ``SVD/LSA`` model directs me to a better way.\n",
    "----\n",
    "* My **target joke** is:\n",
    "\n",
    "    ... And We Wonder Why Everyone Hates Us\n",
    "    \n",
    "    Customer: \"Are you Hispanic?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"Middle Eastern?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"Egyptian?\"\n",
    "    \n",
    "    Me: \"No.\"\n",
    "    \n",
    "    Customer: \"What are you?\"\n",
    "    \n",
    "    Me: \"Chinese.\"\n",
    "    \n",
    "    (customer puts on offended face)\n",
    "    \n",
    "    Customer: \"I don't appreciate you treating me like I'm dumb.\"\n",
    "    \n",
    "    Me: \"Excuse me? I'm being honest.\"\n",
    "    \n",
    "    Customer: \"NO CHINESE PERSON WOULD EVER HAVE EYES AS BIG AS YOURS!!!\"\n",
    "    \n",
    "    Me: *mouth wide open*\n",
    "    \n",
    "    Insults\n",
    "\n",
    "\n",
    "* A **bad recommendation** from ``TF-IDF``:\n",
    "\n",
    "    -------This is how a joke index  173384  looks like:-------\n",
    "\n",
    "     A customer asked me for a good reliable printer...\n",
    "\n",
    "\n",
    "* A **good recommendation** from ``SVD/LSA``:\n",
    "\n",
    "    -------This is how a joke index  117910  looks like:-------\n",
    "\n",
    "     What do Chinese lumberjacks do?\n",
    "\n",
    "     Chopsticks\n",
    "\n",
    "### List of suggestions of how one could improve upon this recommender：\n",
    "* We already see in my data importing cell, there are three different data source and hence three different json dictionary structures. When doing this recommender, I already considered put 'body','title','category' into joke_universe if there are. But notice that, we still have more options to consider, such as 'id','rating'.We can in one way think of adding these two options in our joke universe.\n",
    "* Another way to improve may be to do our own classification first. Since we had some 'category' field from stupidstuff.json and wocka.json, but not from reddit_jokes.json. So, we can import all fields and do a classification to re-classify all of our jokes in to some self-defined categories (more detailed). Taking this new category into consideration when training the model may improve the performance of our recommender.\n",
    "* One trivial but maybe important way is to try different parameters for our initial model instances/objects: ``TfidfVectorizer``, ``TruncatedSVD``."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
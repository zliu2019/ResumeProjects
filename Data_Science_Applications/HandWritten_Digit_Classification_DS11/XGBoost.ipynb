{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mysixenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3d88cacd8b745d4fbed2830424fce0451ea91f459b833c1d75c3abe001a2f0c4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Stochastic Gradient Boosting in XGBoost:\n",
    "\n",
    "[Reference](https://machinelearningmastery.com/stochastic-gradient-boosting-xgboost-scikit-learn-python/)\n",
    "Gradient boosting is a greedy procedure.\n",
    "\n",
    "New decision trees are added to the model to correct the residual error of the existing model.\n",
    "\n",
    "Each decision tree is created using a greedy search procedure to select split points that best minimize an objective function. This can result in trees that use the same attributes and even the same split points again and again.\n",
    "\n",
    "Bagging is a technique where a collection of decision trees are created, each from a different random subset of rows from the training data. The effect is that better performance is achieved from the ensemble of trees because the randomness in the sample allows slightly different trees to be created, adding variance to the ensembled predictions.\n",
    "\n",
    "Random forest takes this one step further, by allowing the features (columns) to be subsampled when choosing split points, adding further variance to the ensemble of trees.\n",
    "\n",
    "These same techniques can be used in the construction of decision trees in gradient boosting in a variation called stochastic gradient boosting.\n",
    "\n",
    "It is common to use aggressive sub-samples of the training data such as 40% to 80%."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "we are going to look at the effect of different subsampling techniques in gradient boosting.\n",
    "\n",
    "We will tune three different flavors of stochastic gradient boosting supported by the XGBoost library in Python, specifically:\n",
    "\n",
    "- Subsampling of rows in the dataset when creating each tree.\n",
    "- Subsampling of columns in the dataset when creating each tree.\n",
    "- Subsampling of columns for each split in the dataset when creating each tree.\n",
    "\n",
    "The goal is to make predictions for new products as an array of probabilities for each of the 10 categories and models are evaluated using multiclass logarithmic loss (also called cross entropy)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Read Data:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "from sklearn.datasets import fetch_openml\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "digit_7 = []\n",
    "for i in mnist['target']:\n",
    "    if i == '7':\n",
    "        digit_7.append(1)\n",
    "    else:\n",
    "        digit_7.append(0)\n",
    "digit_7 = np.array(digit_7)\n",
    "\n",
    "X, y = mnist['data'], digit_7\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "source": [
    "### Tuning Row Subsampling in XGBoost\n",
    "Row subsampling involves selecting a random sample of the training dataset without replacement.\n",
    "\n",
    "Row subsampling can be specified in the scikit-learn wrapper of the XGBoost class in the subsample parameter. The default is 1.0 which is no sub-sampling.\n",
    "\n",
    "We can use the grid search capability built into scikit-learn to evaluate the effect of different subsample values from 0.1 to 1.0 on the Otto dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]"
   ]
  },
  {
   "source": [
    "There are 9 variations of subsample and each model will be evaluated using 10-fold cross validation, meaning that 9Ã—10 or 90 models need to be trained and tested.\n",
    "\n",
    "prints the best configuration as well as the log loss for each tested configuration."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "from matplotlib import pyplot\n",
    "# load data\n",
    "data = read_csv('train.csv')\n",
    "dataset = data.values\n",
    "# split data into X and y\n",
    "X = dataset[:,0:94]\n",
    "y = dataset[:,94]\n",
    "# encode string class values as integers\n",
    "label_encoded_y = LabelEncoder().fit_transform(y)\n",
    "# grid search\n",
    "model = XGBClassifier()\n",
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(subsample=subsample)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1, cv=kfold)\n",
    "grid_result = grid_search.fit(X, label_encoded_y)\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "\tprint(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "# plot\n",
    "pyplot.errorbar(subsample, means, yerr=stds)\n",
    "pyplot.title(\"XGBoost subsample vs Log Loss\")\n",
    "pyplot.xlabel('subsample')\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.savefig('subsample.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "# read in data\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test)\n",
    "# specify parameters via map\n",
    "param = {'max_depth':2, 'eta':1, 'objective':'binary:logistic' }\n",
    "num_round = 2\n",
    "bst = xgb.train(param, dtrain, num_round)\n",
    "# make prediction\n",
    "y_pred = bst.predict(dtest)"
   ]
  },
  {
   "source": [
    "We can see that the best results achieved were 0.3, or training trees using a 30% sample of the training dataset."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}
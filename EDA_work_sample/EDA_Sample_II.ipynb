{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.9 64-bit ('mysixenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3d88cacd8b745d4fbed2830424fce0451ea91f459b833c1d75c3abe001a2f0c4"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h2 id=\"intermediate-image-exploration\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Intermediate Image Exploration</center></h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Image Dimension\n",
    "\n",
    "We shall check if the provided images have the same dimension or not. I'll use Dask to parallelize the operation and speed things up."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get image dimensions\n",
    "def get_dims(file):\n",
    "    img = cv2.imread(file)\n",
    "    h,w = img.shape[:2]\n",
    "    return h,w\n",
    "\n",
    "# parallelize using Dask.bag\n",
    "filelist = train_images_path\n",
    "dimsbag = bag.from_sequence(filelist).map(get_dims)\n",
    "with diagnostics.ProgressBar():\n",
    "    dims = dimsbag.compute()\n",
    "    \n",
    "dim_df = pd.DataFrame(dims, columns=['height', 'width'])\n",
    "sizes = dim_df.groupby(['height', 'width']).size().reset_index().rename(columns={0:'count'})\n",
    "sizes.hvplot.scatter(x='height', y='width', size='count', xlim=(0,1200), ylim=(0,1200), grid=True, xticks=2, \n",
    "        yticks=2, height=500, width=600).options(scaling_factor=0.1, line_alpha=1, fill_alpha=0)"
   ]
  },
  {
   "source": [
    "### K-means Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple k means clustering\n",
    "from sklearn import cluster\n",
    "\n",
    "kmeans = cluster.KMeans(5)\n",
    "clustered = kmeans.fit_predict(pixel_matrix)\n",
    "\n",
    "dims = np.shape(first)\n",
    "clustered_img = np.reshape(clustered, (dims[0], dims[1]))\n",
    "plt.imshow(clustered_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind0, ind1, ind2, ind3 = [np.where(clustered == x)[0] for x in [0, 1, 2, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "plot_vals = [('r', 'o', ind0),\n",
    "             ('b', '^', ind1),\n",
    "             ('g', '8', ind2),\n",
    "             ('m', '*', ind3)]\n",
    "\n",
    "for c, m, ind in plot_vals:\n",
    "    xs = pixel_matrix[ind, 0]\n",
    "    ys = pixel_matrix[ind, 1]\n",
    "    zs = pixel_matrix[ind, 2]\n",
    "    ax.scatter(xs, ys, zs, c=c, marker=m)\n",
    "\n",
    "ax.set_xlabel('Blue channel')\n",
    "ax.set_ylabel('green channel')\n",
    "ax.set_zlabel('Red channel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick look at color value histograms for pixel matrix from first image\n",
    "import seaborn as sns\n",
    "sns.distplot(pixel_matrix[:,0], bins=12)\n",
    "sns.distplot(pixel_matrix[:,1], bins=12)\n",
    "sns.distplot(pixel_matrix[:,2], bins=12)"
   ]
  },
  {
   "source": [
    "### Matching Features"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img79_1, img79_2, img79_3, img79_4, img79_5 = [plt.imread(train_images_path[n]) for n in range(78, 83)]\n",
    "img_list = (img79_1, img79_2, img79_3, img79_4, img79_5)\n",
    "\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(img_list[0])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "Tracking dimensions across image transforms is annoying, so we'll make a class to do that. Also I'm going to use this brightness normalization transform and visualize the image that way, good test scenario for class."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSImage():\n",
    "    \"\"\"Lightweight wrapper for handling image to matrix transforms. No setters,\n",
    "    main point of class is to remember image dimensions despite transforms.\"\"\"\n",
    "    \n",
    "    def __init__(self, img):\n",
    "        \"\"\"Assume color channel interleave that holds true for this set.\"\"\"\n",
    "        self.img = img\n",
    "        self.dims = np.shape(img)\n",
    "        self.mat = np.reshape(img, (self.dims[0] * self.dims[1], self.dims[2]))\n",
    "\n",
    "    @property\n",
    "    def matrix(self):\n",
    "        return self.mat\n",
    "        \n",
    "    @property\n",
    "    def image(self):\n",
    "        return self.img\n",
    "    \n",
    "    def to_flat_img(self, derived):\n",
    "        \"\"\"\"Use dims property to reshape a derived matrix back into image form when\n",
    "        derived image would only have one band.\"\"\"\n",
    "        return np.reshape(derived, (self.dims[0], self.dims[1]))\n",
    "    \n",
    "    def to_matched_img(self, derived):\n",
    "        \"\"\"\"Use dims property to reshape a derived matrix back into image form.\"\"\"\n",
    "        return np.reshape(derived, (self.dims[0], self.dims[1], self.dims[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi79_1 = MSImage(img79_1)\n",
    "print(np.shape(msi79_1.matrix))\n",
    "print(np.shape(msi79_1.img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img79_1)"
   ]
  },
  {
   "source": [
    "### Brightness Normalization\n",
    "Brightness Normalization is preprocessing strategy you can apply prior to using strategies to identify materials in a scene, if you want your matching algorithm to be robust across variations in illumination. See [Wu's paper](https://pantherfile.uwm.edu/cswu/www/my%20publications/2004_RSE.pdf)."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bnormalize(mat):\n",
    "    \"\"\"much faster brightness normalization, since it's all vectorized\"\"\"\n",
    "    bnorm = np.zeros_like(mat, dtype=np.float32)\n",
    "    maxes = np.max(mat, axis=1)\n",
    "    bnorm = mat / np.vstack((maxes, maxes, maxes)).T\n",
    "    return bnorm\n",
    "bnorm = bnormalize(msi79_1.matrix)\n",
    "bnorm_img = msi79_1.to_matched_img(bnorm)\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(bnorm_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msi79_2 = MSImage(img79_2)\n",
    "bnorm79_2 = bnormalize(msi79_2.matrix)\n",
    "bnorm79_2_img = msi79_2.to_matched_img(bnorm79_2)\n",
    "plt.figure(figsize=(8,10))\n",
    "plt.imshow(bnorm79_2_img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(msi79_1.matrix[:,0], bins=12)\n",
    "sns.distplot(msi79_1.matrix[:,1], bins=12)\n",
    "sns.distplot(msi79_1.matrix[:,2], bins=12)"
   ]
  },
  {
   "source": [
    "### Using Thresholds with Brightness Normalization\n",
    "The brightness normalization step is helpful because thresholds that aren't anchored by a preprocessing step end up being arbitrary and can't generalize between scenes even in the same image set, whereas thresholds following brightness normalization tend to pull out materils that stand out from the background more reliably. See the following demonstration:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img79_1[:,:,0] > 230)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(img79_2[:,:,0] > 230)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(bnorm79_2_img[:,:,0] > 0.98)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(bnorm_img[:,:,0] > 0.98)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(121)\n",
    "plt.imshow((bnorm79_2_img[:,:,0] > 0.9999) & \\\n",
    "           (bnorm79_2_img[:,:,1] < 0.9999) & \\\n",
    "           (bnorm79_2_img[:,:,2] < 0.9999))\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(10,15))\n",
    "plt.subplot(121)\n",
    "plt.imshow(bnorm_img[:,:,0] > 0.995)\n",
    "plt.subplot(122)\n",
    "plt.imshow(img79_1)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "### Eigen Images"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing import image\n",
    "\n",
    "# #Add Progressbar\n",
    "# !pip install progressbar\n",
    "\n",
    "# from progressbar import ProgressBar\n",
    "# pbar = ProgressBar()\n",
    "\n",
    "# # making n X m matrix\n",
    "# def img2np(list_of_filename, size = (64, 64)):\n",
    "#     # iterating through each file\n",
    "#     for fp in pbar(list_of_filename):\n",
    "# #         fp = path + fn\n",
    "#         current_image = image.load_img(fp, target_size = size, )\n",
    "# #                                        color_mode = 'grayscale')\n",
    "#         # covert image to a matrix\n",
    "#         img_ts = image.img_to_array(current_image)\n",
    "#         # turn that into a vector / 1D array\n",
    "#         img_ts = [img_ts.ravel()]\n",
    "#         try:\n",
    "#             # concatenate different images\n",
    "#             full_mat = np.concatenate((full_mat, img_ts))\n",
    "#         except UnboundLocalError: \n",
    "#             # if not assigned yet, assign one\n",
    "#             full_mat = img_ts\n",
    "#     return full_mat\n",
    "\n",
    "# # run it on our folder\n",
    "# train_images = img2np(train_images_path)\n",
    "# train_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import PCA\n",
    "# from math import ceil\n",
    "\n",
    "# def eigenimages(full_mat, title, n_comp = 0.7, size = (64, 64)):\n",
    "#     # fit PCA to describe n_comp * variability in the class\n",
    "#     pca = PCA(n_components = n_comp, whiten = True)\n",
    "#     pca.fit(full_mat)\n",
    "#     print('Number of PC: ', pca.n_components_)\n",
    "#     return pca\n",
    "  \n",
    "# def plot_pca(pca, size = (64, 64)):\n",
    "#     # plot eigenimages in a grid\n",
    "#     n = pca.n_components_\n",
    "#     fig = plt.figure(figsize=(8, 8))\n",
    "#     r = int(n**.5)\n",
    "#     c = ceil(n/ r)\n",
    "#     for i in range(n):\n",
    "#         ax = fig.add_subplot(r, c, i + 1, xticks = [], yticks = [])\n",
    "#         ax.imshow(pca.components_[i].reshape(size), \n",
    "#                   cmap='Greys_r')\n",
    "#     plt.axis('off')\n",
    "#     plt.show()\n",
    "    \n",
    "# plot_pca(eigenimages(train_images, 'Eigen Images'))\n",
    "# # plot_pca(eigenimages(pnemonia_images, 'PNEUMONIA'))"
   ]
  },
  {
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h2 id=\"advanced-image-exploration\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Advanced Image Exploration</center></h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Rudimentary Transforms, Edge Detection, Texture"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set144 = [plt.imread(train_images_path[n]) for n in (10000, 11000)]\n",
    "plt.imshow(set144[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.feature import greycomatrix, greycoprops\n",
    "from skimage.filters import sobel"
   ]
  },
  {
   "source": [
    "### Sobel Edge Detection\n",
    "\n",
    "A Sobel filter is one means of getting a basic edge magnitude/gradient image. Can be useful to threshold and find prominent linear features, etc. Several other similar filters in skimage.filters are also good edge detectors: `roberts`, `scharr`, etc. and you can control direction, i.e. use an anisotropic version."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sobel filter is a basic way to get an edge magnitude/gradient image\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(sobel(set144[0][:750,:750,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import sobel_h\n",
    "\n",
    "# can also apply sobel only across one direction.\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.imshow(sobel_h(set144[0][:750,:750,2]), cmap='BuGn')"
   ]
  },
  {
   "source": [
    "### GLCM Textures\n",
    "Processing time can be pretty brutal so we subset the image. We'll create texture images so we can characterize each pixel by the texture of its neighborhood.\n",
    "\n",
    "GLCM is inherently anisotropic but can be averaged so as to be rotation invariant. For more on GLCM, see [the tutorial](http://www.fp.ucalgary.ca/mhallbey/tutorial.htm).\n",
    "\n",
    "A good article on use in remote sensing is [here](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=4660321&url=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D4660321):\n",
    "\n",
    "Pesaresi, M., Gerhardinger, A., & Kayitakire, F. (2008). A robust built-up area presence index by anisotropic rotation-invariant textural measure. Selected Topics in Applied Earth Observations and Remote Sensing, IEEE Journal of, 1(3), 180-192."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = set144[0][:150,:150,2]\n",
    "\n",
    "def glcm_image(img, measure=\"dissimilarity\"):\n",
    "    \"\"\"TODO: allow different window sizes by parameterizing 3, 4. Also should\n",
    "    parameterize direction vector [1] [0]\"\"\"\n",
    "    texture = np.zeros_like(sub)\n",
    "\n",
    "    # quadratic looping in python w/o vectorized routine, yuck!\n",
    "    for i in range(img.shape[0] ):  \n",
    "        for j in range(sub.shape[1] ):  \n",
    "          \n",
    "            # don't calculate at edges\n",
    "            if (i < 3) or \\\n",
    "               (i > (img.shape[0])) or \\\n",
    "               (j < 3) or \\\n",
    "               (j > (img.shape[0] - 4)):          \n",
    "                continue  \n",
    "        \n",
    "            # calculate glcm matrix for 7 x 7 window, use dissimilarity (can swap in\n",
    "            # contrast, etc.)\n",
    "            glcm_window = img[i-3: i+4, j-3 : j+4]  \n",
    "            glcm = greycomatrix(glcm_window, [1], [0],  symmetric = True, normed = True )   \n",
    "            texture[i,j] = greycoprops(glcm, measure)  \n",
    "    return texture\n",
    "\n",
    "    dissimilarity = glcm_image(sub, \"dissimilarity\")\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(dissimilarity, cmap=\"bone\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(sub, cmap=\"bone\")"
   ]
  },
  {
   "source": [
    "### HSV Transform\n",
    "HSV is useful for identifying shadows and illumination, as well as giving us a means to identify similar objects that are distinct by color between scenes (hue), though there's no guarantee the hue will be stable."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import color\n",
    "\n",
    "hsv = color.rgb2hsv(set144[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(set144[0], cmap=\"bone\")\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(hsv[:,:,0], cmap=\"bone\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(hsv[:,:,1], cmap='bone')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(hsv[:,:,2], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(set144[0][:200,:200,:])\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(hsv[:200,:200,0], cmap=\"PuBuGn\")\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(hsv[:200,:200,1], cmap='bone')\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(hsv[:200,:200,2], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 6))\n",
    "plt.imshow(hsv[200:500,200:500,0], cmap='bone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsvmsi = MSImage(hsv)"
   ]
  },
  {
   "source": [
    "### Shadow Detection\n",
    "We can apply a threshold to the V band now to find dark areas that are probably thresholds. Let's look at the distribution of all values then work interactively to find a good filter value."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(hsvmsi.matrix[:,0], bins=12)\n",
    "sns.distplot(hsvmsi.matrix[:,1], bins=12)\n",
    "sns.distplot(hsvmsi.matrix[:,2], bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(hsvmsi.image[:,:,2] < 0.4, cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(set144[0][:250,:250,:])\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(hsvmsi.image[:250,:250,2] < 0.4, cmap=\"plasma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "img2 = plt.imshow(set144[0][:250,:250,:], interpolation='nearest')\n",
    "img3 = plt.imshow(hsvmsi.image[:250,:250,2] < 0.4, cmap='binary_r', alpha=0.4)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h2 id=\"references\" class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:orange; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>References</center></h2>\n",
    "\n",
    "1. [Shopee - Data understanding and analysis](https://www.kaggle.com/isaienkov/shopee-data-understanding-and-analysis) \n",
    "2. [EDA of Shopee for starter](https://www.kaggle.com/chumajin/eda-of-shopee-for-starter)  \n",
    "3. [Shopee Product Matching: EDA+Baseline](https://www.kaggle.com/ruchi798/shopee-product-matching-eda-baseline/data)   \n",
    "4. [Shopee EDA + Understanding Competition!](https://www.kaggle.com/heyytanay/shopee-eda-understanding-competition)    \n",
    "5. [Exploratory Image Analysis](https://www.kaggle.com/bkamphaus/exploratory-image-analysis#Brightness-Normalization)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n",
    "<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:blue; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>More Awesome Plots Coming Soon!</center></h2>"
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}